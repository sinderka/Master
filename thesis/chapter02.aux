\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{zerotransf}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background theory}{4}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Zero initial condition}{4}{section.2.1}}
\newlabel{sec:inittransf}{{2.1}{4}{Zero initial condition}{section.2.1}{}}
\newlabel{eqn:shiftedproblem}{{2.1}{4}{Zero initial condition}{equation.2.1.1}{}}
\citation{energy}
\citation{trapezoidal}
\citation{forwardeuler}
\citation{midpoint}
\citation{symplecticintegrator}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Energy}{5}{section.2.2}}
\newlabel{sec:energy}{{2.2}{5}{Energy}{section.2.2}{}}
\newlabel{eqn:energy2}{{2.2}{5}{Energy}{equation.2.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Integration methods}{5}{section.2.3}}
\citation{expm}
\citation{expm}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Methods for integrating in time. Note that since the midpoint rule uses the midpoint $F_{i+\frac  {1}{2}}$, twice as many points need to be saved for the midpoint rule as for the other methods. The trapezoidal and midpoint rule have quadratic convergence rates, while forward Euler has linear convergence. To compare the methods we use the squared number of points for forward Euler compared to the other methods. $g(t,u)$ is the right hand side of equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eqn:PDE}\unskip \@@italiccorr )}} or \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eqn:PDE1}\unskip \@@italiccorr )}}.\relax }}{6}{table.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:intmet}{{2.1}{6}{Methods for integrating in time. Note that since the midpoint rule uses the midpoint $F_{i+\frac {1}{2}}$, twice as many points need to be saved for the midpoint rule as for the other methods. The trapezoidal and midpoint rule have quadratic convergence rates, while forward Euler has linear convergence. To compare the methods we use the squared number of points for forward Euler compared to the other methods. $g(t,u)$ is the right hand side of equation \eqref {eqn:PDE} or \eqref {eqn:PDE1}.\relax }{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces The method for exact integration in time. Since the method is very computationally demanding it will only be used on small projected matrices. It also need the test problem to have constant energy. The expected convergence will be depending on the approximation of $A$, since this method is only exact in time. The method is explained in MATLAB's docmentation: \cite  {expm}. \relax }}{6}{table.caption.7}}
\newlabel{tab:intcorrect}{{2.2}{6}{The method for exact integration in time. Since the method is very computationally demanding it will only be used on small projected matrices. It also need the test problem to have constant energy. The expected convergence will be depending on the approximation of $A$, since this method is only exact in time. The method is explained in MATLAB's docmentation: \cite {expm}. \relax }{table.caption.7}{}}
\citation{convtrap}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Energy conservation for the trapezoidal rule}{7}{subsection.2.3.1}}
\newlabel{eqn:energyconvinit}{{2.4}{7}{Energy conservation for the trapezoidal rule}{equation.2.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Windowing}{8}{section.2.4}}
\newlabel{sec:windu}{{2.4}{8}{Windowing}{section.2.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  Windowing \relax }}{8}{algorithm.1}}
\newlabel{alg:Kversusk}{{1}{8}{Windowing \relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Solution methods}{8}{section.2.5}}
\newlabel{sec:solmet}{{2.5}{8}{Solution methods}{section.2.5}{}}
\citation{elena}
\citation{min}
\citation{kryprop}
\citation{arnold}
\citation{arnold}
\newlabel{eqn:PMform}{{2.5}{9}{Solution methods}{equation.2.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Arnoldi's Algorithm and the Krylov projection method}{9}{subsection.2.5.1}}
\newlabel{sec:KPM}{{2.5.1}{9}{Arnoldi's Algorithm and the Krylov projection method}{subsection.2.5.1}{}}
\newlabel{eqn:propA}{{2.6}{9}{Arnoldi's Algorithm and the Krylov projection method}{equation.2.5.6}{}}
\newlabel{eqn:stuff}{{2.7}{9}{Arnoldi's Algorithm and the Krylov projection method}{equation.2.5.7}{}}
\citation{arnoldconv}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Arnoldi's algorithm\cite  {arnold}\relax }}{10}{algorithm.2}}
\newlabel{alg:arnoldi}{{2}{10}{Arnoldi's algorithm\cite {arnold}\relax }{algorithm.2}{}}
\newlabel{eqn:Aresidual}{{2.8}{10}{Arnoldi's Algorithm and the Krylov projection method}{equation.2.5.8}{}}
\newlabel{eqn:KPMdiff}{{2.9}{10}{Arnoldi's Algorithm and the Krylov projection method}{equation.2.5.9}{}}
\citation{elenaconv}
\newlabel{eqn:KPMr}{{2.10}{11}{Arnoldi's Algorithm and the Krylov projection method}{equation.2.5.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Symplectic Lanczos method}{11}{subsection.2.5.2}}
\newlabel{sec:SLM}{{2.5.2}{11}{Symplectic Lanczos method}{subsection.2.5.2}{}}
\citation{SLM}
\citation{SLMO}
\citation{SLM}
\citation{SLMO}
\newlabel{eqn:propS}{{2.11}{12}{Symplectic Lanczos method}{equation.2.5.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Symplectic Lanczos method \cite  {SLM}, with reortogonalization from \cite  {SLMO}. \relax }}{12}{algorithm.3}}
\newlabel{alg:symlanz}{{3}{12}{Symplectic Lanczos method \cite {SLM}, with reortogonalization from \cite {SLMO}. \relax }{algorithm.3}{}}
\citation{SLMconv}
\newlabel{eqn:SLMi}{{2.12}{13}{Symplectic Lanczos method}{equation.2.5.12}{}}
\citation{SLMpreserve}
\newlabel{eqn:SLMr}{{2.13}{14}{Symplectic Lanczos method}{equation.2.5.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Proof that \texttt  {SLM} without restart is energy preserving}{14}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Linearity of the methods}{15}{subsection.2.5.3}}
\newlabel{sec:linear}{{2.5.3}{15}{Linearity of the methods}{subsection.2.5.3}{}}
\citation{elena}
\citation{min}
\citation{waveequ}
\citation{min}
\newlabel{eqn:terrible}{{2.14}{16}{Linearity of the methods}{equation.2.5.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}A comment on the restart}{16}{subsection.2.5.4}}
\newlabel{sec:comment}{{2.5.4}{16}{A comment on the restart}{subsection.2.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Direct method}{16}{subsection.2.5.5}}
\newlabel{sec:DM}{{2.5.5}{16}{Direct method}{subsection.2.5.5}{}}
\citation{linearerrorgrowth}
\citation{complex}
\citation{complex}
\citation{SLM1}
\citation{SLM2}
\citation{SLM4}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Computational cost of some mathematical operations. $n$ is restart variable, $\mathaccentV {hat}002{m}$ is the size of the full linear system, and $k$ is the number of steps in time. Computational cost for the different operations are found in \cite  {complex}. \relax }}{17}{table.caption.9}}
\newlabel{tab:cd}{{2.3}{17}{Computational cost of some mathematical operations. $n$ is restart variable, $\hat {m}$ is the size of the full linear system, and $k$ is the number of steps in time. Computational cost for the different operations are found in \cite {complex}. \relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.6}Number of operations}{17}{subsection.2.5.6}}
\citation{SLM1}
\citation{SLM2}
\citation{future}
\citation{benner2016solution}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces  Number of operations needed for the different solving methods when trapezoidal rule is used. $i_r$ is the number of restarts needed for the methods to converge. For windowing $K\cdot k$ is equal to $k$ for the other methods. Windowing with \texttt  {DM} is not interesting since it is exactly the same method as \texttt  {DM}. \relax }}{18}{table.caption.10}}
\newlabel{tab:cc}{{2.4}{18}{Number of operations needed for the different solving methods when trapezoidal rule is used. $i_r$ is the number of restarts needed for the methods to converge. For windowing $K\cdot k$ is equal to $k$ for the other methods. Windowing with \texttt {DM} is not interesting since it is exactly the same method as \texttt {DM}. \relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}SLM as a method for eigenvalue problems}{18}{section.2.6}}
\@setckpt{chapter02}{
\setcounter{page}{20}
\setcounter{equation}{14}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{0}
\setcounter{table}{4}
\setcounter{r@tfl@t}{0}
\setcounter{parentequation}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{19}
\setcounter{ALC@unique}{45}
\setcounter{ALC@line}{24}
\setcounter{ALC@rem}{24}
\setcounter{ALC@depth}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{3}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{section@level}{1}
}
