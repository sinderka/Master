\chapter{ Conclusion }
%The conclusion is, for simplicity, divided in the same two cases that the results were divided into. \\
%The goals with this thesis was to see how the error and energy for the Krylov methods compared 

\noindent The aim with this thesis is was to study two Krylov methods and the restart, and compare them to other popular solution methods in: 
global error, energy preservation, and computation time. The energy preservation for \texttt{SLM} was also examined, together with derivation of the methods and proof of convergence. \\

\noindent The Krylov methods have been compared to \texttt{DM} (see Section \ref{sec:DM}), in error, energy and computation time. Two different Hamiltonian ODEs were used, one was random (See Section \ref{sec:random}), and the other was a discretization of the wave equation (See Section \ref{sec:wave}). Two very different cases were also tested: constant energy, and varying energy. These cases were handled differently. In the case with constant energy, the error was measured with an exact solver, see Table \ref{tab:intcorrect}. When the energy was varying, both error and energy was measured against \texttt{DM}. Because of this, the conclusion is divided into two parts, Section \ref{sec:Konkconst} and \ref{sec:Konkvari}. \\

\noindent The wave equation gave nice results for any $n$, which made the restart uninteresting. To better show how the restarts can be used to improve the solution, only the random test case was used. If you want to use a Krylov method to solve the wave equation, this is very efficient.

\section{Constant energy} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:Konkconst}
The restart can be effectively used to improve the error if a small restart variable (see Section \ref{sec:KPM}) is preferred (see Figure \ref{fig:lcompareErrorw}). \\

\noindent \textbf{When restart is not enabled:} The error for all methods increase linearly with time (see Figure \ref{fig:longtime2err}). \texttt{SLM} is energy preserving (see Figure \ref{fig:vlongene}). \texttt{KPM}s energy is not preserved, but it is bounded. Using an  exact solver instead of the trapezoidal rule gives a smaller error, but does not change the energy (see Figure \ref{fig:idea0}). Both \texttt{SLM} and \texttt{KPM} is faster than \texttt{DM} (see Figure \ref{fig:time0}). This makes \texttt{SLM} an excellent solver on linear Hamiltonian ODEs, when restart is not used.\\

\noindent \textbf{When restart is enabled:} \texttt{SLM} and \texttt{KPM} behave very similar. The error increases linearly with time, but diverges on long time domains (see Figure \ref{fig:SLMenergyerror0} and \ref{fig:vlong}). The energy also increases slowly with time, and diverges on long time domains. Using windowing, the performance of the methods is improved, see Section \ref{sec:windu} and Figure \ref{fig:windowingc}. Using an exact solver instead of the trapezoidal rule gives a smaller error, but does not change the energy (see Figure \ref{fig:idea0}). \texttt{SLM} is very slow, making it less useful  (see Figure \ref{fig:time0}). \texttt{KPM}, on the other hand, is quite fast, but only if the matrix is big, the number of steps in time is small, and the restart variable is well chosen (see Figure \ref{fig:timekr1}). This means that \texttt{KPM} often will be the better alternative of the two methods, when restart is used. \\

\noindent Using windowing and/or an exact solver does not change the computation time, see Figure \ref{fig:time2} and \ref{fig:time3}.
\section{Varying energy} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:Konkvari}

For \texttt{SLM} and \texttt{KPM} without restart, the energy and error are bounded, but not constant (see Figure \ref{fig:vSLMenergyerror1}). When restart is enabled, the energy and error increases slower, but diverges on long time domains (see Figure \ref{fig:vSLMenergyerror0}). The divergence cannot be removed by windowing (see Figure \ref{fig:lversuskenergy}). Compared to \texttt{DM}, \texttt{SLM} is slow, while \texttt{KPM} is fast if the size of the matrix is large, the number of time steps is small, and $n$ is well chosen (see Figure \ref{fig:ltime0}). This makes \texttt{KPM} the better choice if this is the case, and \texttt{DM} better in the other cases.

\section{Further work}
It could be interesting to see how the error and energy behave with a random Hamiltonian matrix with different structures (eg. full random, sparse 5-diagonal, 1-diagonal), with a known analytical solution (eg. make an accurate eigenvalue/eigenvector decomposition of the matrix and then take the exponential of the diagonal matrix of eigenvalues, or use the method based on the shur decomposition).
This could shed more light on when and how to best utilize the Krylov methods.\\

\noindent The Krylov methods ability to create small matrices can make it possible to utilize graphics cards as the processing unit. Graphics cards are designed to be used on small matrices \cite{graphics}. They consists of thousands of processing units, which could be used to solve non linear Hamiltonian problems in parallel (see Section \ref{sec:linear}).
