%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This chapter will contain the theoretical aspects of the projection methods, such as derivation, proof of convergence and assumptions. 
 
\section{Zero initial condition}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:inittransf}
For both \texttt{KPM} and \texttt{SLM} it is important that the initial conditions are zero. The reason for this is explained in section \ref{sec:solmet}. Equation \eqref{eqn:PDE} can be transformed so that it has zero initial conditions in the following way \cite{zerotransf}: \\
Start by shifting the solution
\begin{equation*}
\hat{u}(t) = u(t)-u_0,
\end{equation*}
then rewrite the original equation as
\begin{equation*}
\begin{aligned}
\dot{\hat{u}}(t) &= A \hat{u}(t) +A u_0\\
 \hat{u}(0)&= 0. \\
\end{aligned}
\end{equation*}
The equation above solves the shifted problem, solve the original problem by shifting it back with
\begin{equation*}
 u(t) = \hat{u} + u_0. \\
\end{equation*}


All test problems with a non-zero initial condition will be transformed in this way without using the hat notation. The letter $b$ will be used as a collective term to describe both $A u_0$, or any constant vector occurring on the right hand side of equation \eqref{eqn:PDE}.

\section{Energy}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
It is well known that the energy of a system on the form of equation \eqref{eqn:PDE} can be expressed as \cite{energy}
\begin{equation*}
\begin{aligned}
\mathcal{H}_1(u) = \frac{1}{2} u^\top (t) J A u(t)
\end{aligned}
\end{equation*}
If the transformation in section \ref{sec:inittransf} is used, the energy is 
\begin{equation}
\mathcal{H}_2 (\hat{u}) = \frac{1}{2} \hat{u}^\top (t)  J A \hat{u}(t) + \hat{u}^\top (t)  J b.
\label{eqn:energy2}
\end{equation}
For most theoretical derivation $\mathcal{H}_2$ will be used, as the shifted problem is the one actually solved. But in figures $\mathcal{H}_1$ is used since the unshifted problem is what is sought. \\

There will be a discussion about different types of test problem. The ones marked "constant energy" will be problems without a time dependent source term, meaning. The other type is problems marked "varying energy", in this case there will be a non zero time dependent scalar function $f(t)$, multiplied with a constant vector $p$ in equation \eqref{eqn:PDE}. Test problems will be presented in section \ref{sec:testprob}.

\section{Integration methods}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The time domain $[0,T_s]$ will be divided in $k$ pieces, so that the step size is constant $h_t = T_s/k$. The time discretized solution of $u(t_j)$ will be called $U_j$, with $t_j = j h_t$ where $ j = 1,2,\cdots,k $, $j = 0$ is disregarded since the initial value is known to be zero. Let the discretization of $f(t_j)$ be called $F_j$. The integration methods considered in this text are trapezoidal rule, forward Euler, and midpoint rule. The definition and the iteration scheme of the different methods are given in table \ref{tab:intmet}. \\

\begin{table}
\begin{tabular}{l l}
	Trapezoidal rule (\texttt{trap}) \cite{trapezoidal} & $U_{i+1} = U_{i}+h_t g \Big( \frac{1}{2}(t_i+t_{i+1}),\frac{1}{2}(U_i+U_{i+1}) \Big)$
	\\ & $U_{i+1} = (I- \frac{A h_t}{2}) ^{-1} \Big(  U_i + \frac{h_t}{2} \big( A U_i+(F_{i+1}+F_i) \big)  \Big) $\\
\hline	
	Forward Euler (\texttt{Euler}) \cite{forwardeuler} & $ U_{i+1} = U_i + h_t g ( t_i, U_i ) $ \\ & $ U_{i+1} = U_i + h_t \big( A U_i + F_i \big) $ \\
	\hline
	Midpoint rule (\texttt{mid}) \cite{midpoint} & $U_{i+1} = U_i + h_t g \Big(  t_{i+\frac{1}{2}} , \frac{1}{2}(U_i + U_{i+1})    \Big) $ \\ & 
	$U_{i+\frac{1}{2}} = U_i + \frac{h_t}{2} ( A U_i + F_{\frac{1}{2}} )$ \\ &
    $U_{i+1} = (I-\frac{A h_t}{2}) ^{-1} (U_{i+\frac{1}{2}} + \frac{h_t}{2} F_{i+ \frac{1}{2}})$
    
    
\end{tabular}

\caption{Methods for integrating in time. Note that since the midpoint rule uses the midpoint $F_{i+\frac{1}{2}}$, twice as many points needs to be saved for midpoint rule as for the other methods. Trapezoidal and midpoint rule have quadratic convergence rates, while forward Euler has linear convergence. To compare the methods it is therefore necessary to use the squared number of points for forward Euler as for the other methods.} 
\label{tab:intmet}
\end{table}
Trapezoidal rule and midpoint rule is the same method if the energy is constant. When $F_j$  is not constant, midpoint rule should have an advantage because it is symplectic \cite{symplecticintegrator}. \\

Forward Euler has no energy preserving properties, and is used to show the difference between a naive integration method, and energy preserving integration methods. \\

In addition to the iteration schemes in table \ref{tab:intmet} some exact solvers are used, they are presented in table \ref{tab:intcorrect}.
\begin{table}
\begin{tabular}{l l}
Eigenvalue and diagonalization (\texttt{diag}) & $[V,D] = \texttt{eig}(A)$ \\
 & $U_i = V \cdot \texttt{diag} \Big( \texttt{exp} \big( \texttt{diag}(D \cdot t_i)\big)\Big)/V \cdot b - b$ \\
Matlab's \texttt{expm} function (\texttt{expm}) & $U_i = \texttt{expm}(A \cdot t_i) \cdot b - b$ \\

\end{tabular}
\caption{Methods for exact integration in time. Since they are very computationally demanding they will only be used on small projected matrices. They also need the test problem to have constant energy. The expected convergence will be depending on the approximation of $A$, since this method is only exact in time. These function as explained in matlabs docmentation: \cite{expm}. }
\label{tab:intcorrect} 
\end{table}

\subsection{Energy conservation for trapezoidal rule} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This section will show the energy preserving properties of trapezoidal rule on the initial value shifted function
\begin{equation}
\begin{aligned}
\dot{u}(t)& = Au + b \\
u(0)& = 0.
\end{aligned}
\label{eqn:energyconvinit}
\end{equation}
Note that $b$ can be any constant vector, not just $A u_0$. The proof is based on \cite{convtrap}.
The energy of this function has already been presented in equation \eqref{eqn:energy2}. The main ingredients in this proof is the gradient of $\mathcal{H}_1$, and the iterations scheme for the trapezoidal rule. Assume that $A$ is a Hamiltonian matrix, so that $JA$ i symmetric. The gradient of $\mathcal{H}_1(u)$ is 
\begin{equation*}
\begin{aligned}
\nabla \mathcal{H}_1(u) = J (Au + b) .
\end{aligned}
\end{equation*}
The trapezoidal rule found in table \ref{tab:intmet}, used on equation \eqref{eqn:energyconvinit} gives 
\begin{equation*}
\begin{aligned}
\frac{U_{j+1} - U_j}{h_t} = A \frac{U_{j+1}  + U_j}{2} + b.
\end{aligned}
\end{equation*}
Substituting $\frac{U_{j+1}  + U_j}{2} $ for $u$ gives the gradient of the energy of this function
\begin{equation*}
\begin{aligned}
\nabla \mathcal{H}_1(\frac{U_{j+1}  + U_j}{2}) = JA \frac{U_{j+1}  + U_j}{2} + J b
\end{aligned}
\end{equation*}
Since
%\begin{equation*}
$\frac{ U_{j+1} - U_j}{h_t} = J^{-1} \nabla \mathcal{H}_1( \frac{U_{j+1}  + U_j}{2} ) $
%\end{equation*} 
and
%\begin{equation*}
$\nabla \mathcal{H}_1(\frac{U_{j+1}  + U_j}{2})^\top J^{-1} \nabla \mathcal{H}_1(\frac{U_{j+1}  + U_j}{2}) = 0$
%\end{equation*}
we have
\begin{equation*}
\begin{aligned}
\nabla \mathcal{H}_1 (\frac{U_{j+1}  + U_j}{2}) ^\top \frac{U_{j+1} - U_j}{ h_t } = 0
\end{aligned}
\end{equation*}
Substituting $ J (A\frac{U_{j+1}  + U_j}{2}  + b) $ for $\nabla \mathcal{H}_1 (\frac{U_{j+1}  + U_j}{2})$, and solving the parenthesis gives
\begin{equation*}
\begin{aligned}
\mathcal{H}_1(U_{j+1}) - \mathcal{H}_1(U_{j}) = 0
\end{aligned}
\end{equation*}
So trapezoidal rule conserves the energy for functions with constant energy. Since the initial conditions are satisfied the energy will have the correct value at all times.

\section{Windowing}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As you will see, the projected methods has a tendency to stop working when the time domain is too large. A fix to this might be to divide the time domain in smaller pieces, and solve each piece individually. How this is done is described in the next paragraph.\\ 

Let $T_s$ denote simulated time, let $K$ be the number of pieces $T_s$ is divide into, and let $k$ be the number of pieces each $K$ is divided into. Each of the $K$ sub intervals is then solved as separate problems, with the initial conditions updated. This is explained in a more precise manner in algorithm \ref{alg:Kversusk}. This method will be called "windowing", as this is the name my supervisor Elena Celledoni \cite{elenaperson} used to describe the method.

\begin{algorithm} [h!]
\begin{algorithmic} \caption{ Windowing } \label{alg:Kversusk}  
\STATE Start with an initial value $U_0$, $K$ and $k$.
\STATE Make an empty vector $U$.
\FOR{$j = 1,2,\cdots, K $} 
   \STATE Solve differential equation with $k+1$ points in time and initial value $U_0$.
   \STATE Place the new points at the end of $U$.
   \STATE Update $U_0$ to be the last value of $U$.
   \STATE Delete the last point of $U$.
\ENDFOR
\STATE Return $U$.
\end{algorithmic} 
\end{algorithm}

\section{Solution methods} \label{sec:solmet} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
There are two orthogonalisation methods that will be discussed in this text. Their names are symplectic Lanczos method(\texttt{SLM}), and Arnoldi's algorithm(\texttt{KPM}, as it is implemented as the Krylov projection method). The derivation of the methods, together with a proof of energy perseverance for \texttt{SLM} is presented in this section. \\


Projection methods are ways to make approximated solutions from a subset of the original problem by projecting a problem of several dimensions onto a smaller dimensional problem. One great feature of these methods is that a smaller system of equations can be used to obtain solutions, making further computations less demanding, but finding this projected system can be time consuming. An other drawback is that the projected solutions are approximations. The approximated solution can be improved by something called restarting, this means using the projection method again, to solve an equation for the difference between the projected solution, and the unprojected solution. This can be done repeatedly to obtain the desired accuracy. \\

Assume that the equations are on the form
\begin{equation}
\begin{aligned}
\dot{u}(t) &= Au(t) + b \\
u(0) &= 0
\end{aligned}
\label{eqn:PMform}
\end{equation}
when these methods are used. Note that the initial values are zero as the restarts considers the difference between the projected solution and the unprojected solution, since it is difficult to know how well the initial value is approximated. It is also important that $A$ is a Hamiltonian matrix when using \texttt{SLM}. \\

Note that the relation between $\hat{m}$, $\tilde{n}$, $\tilde{m}$ used in the algorithms is given by $\hat{m} = 2\tilde{m}= 2(m-2)^2$ and $ n = 2\tilde{n}$, where $\hat{m}$ is the dimension of the system, and $n$ is the size of the orthogonal system. $n$ is sometimes called the restart variable. Don't worry to much about these details, it is just a way to simplify the expressions, the reason to simplify them this way will be described in section \ref{sec:testprob}. \\

When talking about the true solution it is the meant as the unprojected solution(the same problem, solved exactly the same way, except without any projection method), and will later be referred to as the direct method(\texttt{DM}).
\subsection{Arnoldi's Algorithm and the Krylov projection method} %%%%%%%%%%%%%%%%%
This section is loosely based around the derivation of the method done in \cite{elena} and \cite{min}. \\

The Krylov subspace is the space $W_n (A,b) = \{b,Ab, \cdots, A^{n-1}b\} = \{v_1,v_2,\cdots,v_n\} $, where $n \leq \hat{m}$.
The vectors $v_i$ together with $h_{i,j} = v_i^\top Av_j$, are found by using Arnoldi's algorithm, shown in algorithm \ref{alg:arnoldi}. Let $V_n$ be the $\hat{m} \times n$ matrix consisting of column vectors $[v_1,v_2,\cdots,v_n ] $ and $H_n$ be the $n \times n$ upper Hessenberg matrix containing all elements $(h_{i,j})_{i,j=1,\cdots,n}$. Then the following holds \cite{kryprop}

\begin{equation}
\begin{aligned}
AV_n & = V_n H_n + h_{n+1,n}v_{n+1}e^\top_n  \\
V^{\top}_n AV_n &= H_n  \\
V_n^{\top} V_n &= I_n. 
\label{eqn:propA}
\end{aligned}
\end{equation}

\begin{algorithm} [h!]
\begin{algorithmic} \caption{Arnoldi's algorithm\cite{arnold}} \label{alg:arnoldi}  
\STATE Start with $A \in \mathbb{R}^{\hat{m} \times \hat{m}}$, $b \in \mathbb{R}^{\hat{m}}$, $n \in \mathbb{N}$ and a tolerance $\iota \in \mathbb{R}$.
\STATE $v_1 = b/\|b \|_2$
\FOR{$j = 1,2,\cdots, n $} 
   \STATE Compute $h_{i,j} =  v_i^{\top}Av_j,v_i $ for $i = 1,2,\cdots, j$
    \STATE Compute $w_j = A v_j - \Sigma_{i=1}^{j} h_{i,j}v_i $
    \STATE $h_{j+1,j} = \| w_j \|_2$
    \IF{$h_{j+1,j} < \iota $} %
        \STATE\textbf{STOP}
    \ENDIF 
   \STATE $v_{j+1} = w_j/h_{j+1,j}$
\ENDFOR
\STATE Return $H_n$, $V_n$, $v_{n+1}$, $h_{n+1,n}$.
\end{algorithmic} 
\end{algorithm}



Here, $e_n$ is the $n$th canonical vector in $\mathbb{R}^n$. $n$ is the number of iterations performed with Arnoldi.\\

By using the transformation $u(t) \approx V_n z_n(t)$, equation \eqref{eqn:PMform} can, with the help of equation \eqref{eqn:propA}, be written as

\begin{equation}
\begin{aligned}
\dot{z}_n(t) &= H_n z_n(t) + \| b\|_2 e_1 f(t)  \\
z_n(0) &= 0.
\label{eqn:KPMi}
\end{aligned}
\end{equation}
The approximated solution of the original problem can be attained by the following relation
\begin{equation*}
u_n(t) = V_n z_n(t),
\end{equation*}
where $u_n$ is the (approximated) solution found with $n$ as a restart variable. \\

Convergence for the method can be shown by finding an expression for the residual, which can be written as
\begin{equation*}
r_n(t) = v f(t) -\dot{u}_n(t) + A u_n(t)
\end{equation*}
This can be rewritten with $u_n(t) = V_n z_n(t)$, to get
\begin{equation*}
r_n(t) = v f(t) - V_n \dot{z}_n(t) + A V_n z_n(t)
\end{equation*}
By equation \eqref{eqn:propA} it can be simplified to
\begin{equation}
r_n(t) = h_{n+1,n} e_n^\top z_n(t) v_{n+1}
\label{eqn:Aresidual}
\end{equation}
Since $h_{n+1,n}$ is zero for some $n \leq \hat{m}$ (since $V_{\hat{m}} h_{n+1,n} v_{n+1} = 0 $ by construction ), the procedure will converge toward the correct solution $u(t)$. \\

Larger $n$ gives better a approximation of the solution, but larger $n$ also gives higher computational complexity. The drawback is also that there is no way of knowing how well the solution is approximated in advance, but the size of $h_{n+1,n}$ does say something about how well the solution is approximated( smaller $h_{n+1,n}$ means a smaller error ), though it is only available after solving the problem. If the approximation is not sufficient the solution must be recalculated with larger $n$, unless you perform a restart. \\

A restart considers the difference $\epsilon_n^{(i)}(t) = u(t)-u_n^{(i)}$, as in equation \eqref{eqn:KPMdiff}. The iteration variable $i$ is present since it is possible to restart several times, $u_n^{(i)}$ is the solution obtained after $i$ restarts with $n$ as a restart variable. 
\begin{equation}
\begin{aligned}
\dot{\epsilon}_n^{(i)}(t)=A \epsilon_n^{(i)} (t)  - r_n^{(i)}.
\end{aligned}
\label{eqn:KPMdiff}
\end{equation}
Where 
\begin{equation*}
r_n^{(i)} = h_{n+1,n}^{(i-1)} v_{n+1}^{(i-1)} e_n^{\top} \epsilon_n^{(i-1)} (t),
\end{equation*}
which is exactly the same as in equation \eqref{eqn:Aresidual}, except that $z_n(t)$ is replaced with $\epsilon_n(t)$, and the counter $i$ is present. 
Equation \eqref{eqn:KPMdiff} can be simplified by using equation \eqref{eqn:propA}, and writing $ \epsilon^{(i)}_n(t)  = V_n \delta_n^{(i)}(t) $, to obtain equation \eqref{eqn:KPMr}.
\begin{equation}
\dot{\delta}^{(i)}_n(t) = H_n^{(i)} \delta_n^{(i)}(t) + e_1 h_{n+1,n}^{(i-1)} e^\top_n \delta_n^{(i-1)}(t), \quad i \geq 1
\label{eqn:KPMr}
\end{equation}
The solution is found by $ u_n^{(i)}(t) = \sum \limits_{j = 0} ^i S_n^{(j)} \delta_n^{(j)} (t) $, where $\delta_n^{(0)} (t) = z_n(t)$ and found by equation \eqref{eqn:KPMi}. $H_n$, $v_{n+1}$, $h_{n+1,n}$ and $V_n$ without counting variables will also be referring to equation \eqref{eqn:KPMi}. \\
Repeatedly solving this equation can increase the accuracy of the approximated solution within an arbitrary constant of the desired solution. It is no longer possible to use $h_{n+1,n}$ as a measure for the error when the restart is used, since Arnoldi's algorithm has no way to measure how much this iteration improved the solution compared to previous iterations.\\

The proof for the convergence of the restart can be found in \cite{elenaconv}.\\
\subsection{Symplectic Lanczos method}
\texttt{SLM} and \texttt{KPM} are very similar, which is easy to see when comparing equation \eqref{eqn:propA} and \eqref{eqn:propS}. The main difference is that orthonormality in Arnoldi is replaced by symplecticity in \texttt{SLM}. This makes the derivation quite similar.\\

Let $S_n = [v_1,v_2,\cdots v_{\frac{n}{2}},w_1,w_2,\cdots w_{\frac{n}{2}}]$ be a set of $J$-orthogonal vectors satisfying the following equations \cite{SLMprop}
\begin{equation}
\begin{aligned}
AS_n &= S_n H_n + \zeta_{n+1} v_{n+1} e_{\hat{m}}^\top\\
J_{n}^{-1} S_n^\top J_{\hat{m}} A S_n &= H_n \\
S_n^{\top} J_{\hat{m}} S_n &= J_{n}\\
\label{eqn:propS}
\end{aligned}
\end{equation}
\begin{algorithm} \caption{Symplectic Lanczos method \cite{SLM}, with reortogonalization from \cite{SLMO}. } \label{alg:symlanz}
\begin{algorithmic}
\STATE Start with a Hamiltonian matrix $A \in \mathbb{R}^{2\tilde{m} \times 2 \tilde{m}}$, $b \in \mathbb{R}^{2 \tilde{m}}$, $\tilde{n} \in \mathbb{N}$
\STATE $v_0= 0 \in \mathbb{R}^{2 \tilde{m}}$
\STATE $\zeta_1 = \| b\|_2$
\STATE $v_1= \frac{1}{\zeta_1}  b$
\FOR {$j = 1,2, \cdots, \tilde{n}$}
	\STATE $v = A v_j$
	\STATE $\delta_j =  v_j^\top v$
	\STATE $\tilde{w} = v-\delta_j v_j$
	\STATE $\kappa_j = v_j^\top J_{\tilde{m}} v $
	\STATE $w_j = \frac{1}{\kappa_j} \tilde{w_j}$
	\STATE $w = A w^j$
	\STATE $ \tilde{S}_{j-1} = [v_1,v_2,\cdots,v_{j-1},w_1,w_2,\cdots,w_{j-1}] $
	\STATE $ w_j = w_j + \tilde{S}_{j-1}J_{j-1} \tilde{S}_{j-1}^\top J_{\tilde{m}} w_j $
	\STATE $\beta = -w_j^\top J_{\tilde{m}} w$
	\STATE $\tilde{v}_{j+1} = w - \zeta_j v_{j-1} - \beta_j v_j + \delta_j v_j$
	\STATE $ \zeta_{j+1} = \|\tilde{v}_{j+1} \|_2 $
	\STATE $ v_{j+1} = \frac{1}{\zeta_{j+1}} \tilde{v}_{j+1} $
	\STATE $ \tilde{S}_j = [v_1,v_2,\cdots,v_{j},w_1,w_2,\cdots,w_{j}] $
	\STATE $ v_{j+1} = v_{j+1} + \tilde{S}_j J_j \tilde{S}_j^\top J_{\tilde{m}} v_{j+1} $
\ENDFOR
\STATE $S_n = [v_1,v_2,\cdots,v_{\tilde{n}},w_1,w_2,\cdots,w_{\tilde{n}}]$
\STATE $H_n = \begin{bmatrix}
\text{diag} \big( [\delta_j]^{\tilde{n}}_{j=1} \big) & \text{tridiag}\big( [\zeta_j]_{j=2}^{\tilde{n}},[\beta_j]_{j=1}^{\tilde{n}},[\zeta_j]_{j=2}^{\tilde{n}} \big) \\
\text{diag} \big( [\kappa_j]^{\tilde{n}}_{j=1} \big) & \text{diag} \big( [-\delta_j]^{\tilde{n}}_{j=1} \big)
\end{bmatrix} $
\STATE Return $H_n$, $S_n$, $v_{n+1}$, $\zeta_{n+1}$.
\end{algorithmic}
\end{algorithm}


Here $S_n$ is an $\hat{m} \times n $ matrix, $H_n$ is an $ n \times n $ matrix, where $ \tilde{n} = \frac{n}{2}$ is the number of iterations the algorithm performed, this is because the algorithm makes two vectors per iterations: $v_j$ and $w_j$. \\

The process of making the vectors and matrix is a little more involved than for Arnoldi's algorithm, so there will be no thorough explanation of how it works, except for in Algorithm \ref{alg:symlanz}.\\

Transform the problem in equation \eqref{eqn:PMform} with $u(t) \approx S_n z_n(t)$, multiply with $J^{-1}_n S_n^\top J_{ \hat{m} }$, and simplify with equation \eqref{eqn:propS} to obtain
\begin{equation*}
\begin{aligned}
\dot{z}(t) = H_n z_n(t) + J_n^{-1} S_n^\top J_{\hat{m}} b f(t).
\end{aligned}
\end{equation*}
This can be further simplified when writing $ b = S_n e_1 \| b \|_2 $ and using equation \eqref{eqn:propS}:
\begin{equation}
\begin{aligned}
\dot{z}(t) = H_n z_n(t) + \|b \|_2 e_1 f(t).
\end{aligned}
\label{eqn:SLMi}
\end{equation}
Equation \eqref{eqn:SLMi} and \eqref{eqn:KPMi} are identical, except for the orthogonalization method used. \\

Since $ S_{\hat{m}}^\top J_{\hat{m}} \zeta_{n+1} v_{n+1} = 0 $ by construction \cite{SLMconv}, the proof of convergence will be very similar to the proof for \texttt{KPM}.\\
The residual of the method can be written as
\begin{equation*}
r_n(t) = v f(t) - \dot{u}_n(t) A u_n(t).
\end{equation*}
This can rewritten with $u_n = S_n z_n(t)$.
\begin{equation*}
r_n(t) = v f(t) -S_n \dot{z}_n(t) + A S_n z_n(t).
\end{equation*}
By equation \eqref{eqn:propS} it becomes
\begin{equation*}
r_n(t) =  \zeta_{n+1} v_{n+1} e_{\hat{m}}^\top z(t).
\end{equation*}
Since $\zeta_{n+1}$ will approach zero as $n$ grows, $r_n$ will approach zero, and the method will converge. \\

A restart can also here be performed if the accuracy of the solution is not satisfactory. This can be derived by looking at the difference $ \epsilon_n = u(t) - S_n z_n(t)$: %The result is given in equation \eqref{eqn:SLMr}. 
\begin{equation*}
\dot{\epsilon}_n^{(i)}(t) = A \epsilon_n^{(i)}(t) + r_n^{(i)}(t),
\label{eqn:resenerg}
\end{equation*}
where
\begin{equation*}
r_n^{(i)}(t) = \zeta_{n+1}^{(i-1)} v_{n+1}^{(i-1)} e_{\hat{m}}^\top \epsilon_n^{(i-1)}(t).
\end{equation*}

Write $ \epsilon^{(i)}_n(t)  = V_n \delta_n^{(i)}(t) $ to obtain
\begin{equation*}
\begin{aligned}
\dot{\delta}_n^{(i)} = H_n^{(i)} \delta_n^{(i)} + J^{-1}_n {S_n^{(i)}}^\top J_{\hat{m}} \zeta_{n+1}^{(i-1)}v_{n+1}^{(i-1)} e_n^\top \delta_n^{(i-1)}.
\end{aligned}
\end{equation*}

This can be further simplified to
\begin{equation*}
\begin{aligned}
\dot{\delta}_n^{(i)} = H_n^{(i)} \delta_n^{(i)} + e_1 \zeta_{n+1}^{(i-1)} e_n^\top \delta_n^{(i-1)}, \quad i \geq 1.
\label{eqn:SLMr}
\end{aligned}
\end{equation*}
The solution is found by $ u_n^{(i)}(t) = \sum \limits_{j = 0} ^i S_n^{(j)} \delta_n^{(j)} (t) $, where $\delta_n^{(0)} (t) = z_n(t)$ and found by equation \eqref{eqn:SLMi}. $H_n$, $v_{n+1}$, $\zeta_{n+1}$ and $S_n$ without counting variables will also be referring to equation \eqref{eqn:SLMi} \\

Proof of convergence and other interesting results for this method can be found in \cite{SLMinteresting}. 

\subsubsection{Proof that \texttt{SLM} without restart is energy preserving} %%%%%%%%%%%%%%%%%%%%%%
If equation \eqref{eqn:SLMi} is solved by an energy preserving method, eg. trapezoidal rule, the energy will be preserved \cite{SLMpreserve}. The energy of this equation is
\begin{equation*}
\mathcal{H}(z_n) = \frac{1}{2}z_n(t)^\top J_n H_n z_n(t) + z_n(t)^\top J_n e_1 \|b \|_2
\end{equation*}

While the energy of the original problem is 
\begin{equation*}
\mathcal{H}(u_n) = \frac{1}{2}u_n(t)^\top J A u_n(t) + u_n(t)^\top J b.
\end{equation*}
Perform the substitution $ u_n(t) = S_n z_n(t) $ and get
\begin{equation*}
\mathcal{H}(u_n) = \frac{1}{2}z_n(t)^\top S_n^\top J A S_n z_n(t) + z_n(t)^\top S_n^\top J b
\end{equation*}
use that $ b = S_n e_1 \| b \|_2 $, and simplify with equation \eqref{eqn:propS}.
\begin{equation*}
\mathcal{H}(u_n) = \frac{1}{2}z_n(t)^\top J_n H_n z_n(t) + z_n(t)^\top J_n e_1 \|b \|_2
\end{equation*}
And we see that 
\begin{equation*}
\mathcal{H}(z_n) - \mathcal{H}(u_n) = 0
\end{equation*}
So the transformation does not change the energy, thus \texttt{SLM} is energy preserving. This will not hold for \texttt{KPM} for a couple of reasons. First $H_n$ is not a Hamiltonian matrix, so the method it self is not energy preserving. The other reason is that the transformation $V_n$ is not symplectic.\\ 
\texttt{SLM} with restart is also not energy preserving since the error equation is depending on time, which ruins the energy preserving properties. But this property can be somewhat regain by restarting several times.
\subsubsection{Residual energy}
Equation \eqref{eqn:energy3} and \eqref{eqn:energy4} are solemnly used to describe the residual energy of the methods. $\mathcal{H}_3$ describes the residual energy of the projected method, in $u_n$ space, that is, after transforming back from $z_n$ space. $\mathcal{H}_4$ is the residual energy in $z_n$ space. In theory these energies should be equal and zero. Section \ref{sec:residualenergy} shows how this holds up in practice. 
They are presented in the equation below, and can easily be derived from \eqref{eqn:resenerg} and \eqref{eqn:SLMr}. \\
\begin{equation}
\mathcal{H}_3 (t) = \frac{1}{2} {\epsilon^{(1)}}^\top (t) J_m A \epsilon^{(1)} + {\epsilon^{(1)}}^\top J_m h_{n+1,n}^{(1)} v_{n+1}^{(1)} e_n^\top z(t)
\label{eqn:energy3}
\end{equation}

\begin{equation}
\mathcal{H} (t) = \frac{1}{2} {\delta^{(1)}}^\top (t) J_n H_n^{(2)} \delta^{(1)} + {\delta^{(1)}}^\top {S_n^{(2)}}^\top  J_m h_{n+1,n}^{(1)} v_{n+1}^{(1)} e_n^\top z(t)
\label{eqn:energy4}
\end{equation}
The iteration variables are present since it considers the residual with one restart.

\subsection{A comment on the orhogonalisation methods} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The  derivation of the methods above shows the vast similarities between \texttt{SLM} and \texttt{KPM}. The only practical difference between the two methods are the orthogonalisation methods. This makes implementation of the methods very similar.  \\

How to obtain the correct number of restarts to get a good estimate of the solution will be discussed in section \ref{sec:praktisk} and \ref{sec:resultat}.

\subsection{Direct method} \label{sec:DM}
It is important to have some method to compare with these projection methods with. This will be done by solving the problem exactly the same way, but without using any projection method, this will be called direct method, or \texttt{DM}. \\

The energy of \texttt{DM} will be constant if an energy preserving method is used, while the error will increase linearly \cite{linearerrorgrowth}. A goal of this text is to see how \texttt{KPM} and \texttt{SLM}'s error and energy behaves compared to this. 

The proof that \texttt{DM} is the natural method to compare with will be shown here with trapezoidal rule for \texttt{KPM}. The proof is very similar for \texttt{SLM}. \\

We start by stating the desired equation discretized with the trapezoidal rule:
\begin{equation*}
\begin{aligned}
(I-\frac{Ah_t}{2}) U_{i+1} = \Big( U_i + \frac{h_t}{2} \big( A U_i + F_{i+1} +F_{i} \big).
\end{aligned}
\end{equation*}
Applying the transformation $ U_i = V_n Z_i(t) $ gives
\begin{equation*}
\begin{aligned}
(V_n-\frac{A V_n h_t}{2}) Z_{i+1} = \Big( V_n Z_i + \frac{h_t}{2} \big( A V_n Z_i + F_{i+1} +F_{i} \big).
\end{aligned}
\end{equation*}
Using equation \eqref{eqn:propA} gives
\begin{equation*}
\begin{aligned}
\Big(V_n-h_t\frac{V_n H_n + h_{n+1,n}v_{n+1} e_n^\top }{2}\Big) Z_{i+1} = \Big( V_n Z_i + \frac{h_t}{2} \big( (V_n H_n + h_{n+1,n}v_{n+1} e_n^\top) Z_i + F_{i+1} +F_{i} \big) \Big).
\end{aligned}
\end{equation*}
When using the trapezoidal rule directly on \eqref{eqn:KPMi}, the result is
\begin{equation*}
\begin{aligned}
\Big(V_n-h_t\frac{V_n H_n}{2}\Big) Z_{i+1} = \Big( V_n Z_i + \frac{h_t}{2} \big( V_n H_n Z_i + F_{i+1} +F_{i} \big) \Big).
\end{aligned}
\end{equation*}
If we now let $n$ grow, $h_{n+1,n}$ will become zero and the projected method will converge towards the unprojected method. 
\subsection{Linearity of the methods} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The projection methods require a vector that can be used to generate the orthogonal space. In the general problem,
\begin{equation*}
\begin{aligned}
\dot{u}(t) &= Au(t) + b,
\end{aligned}
\end{equation*} 
$b$ is used to create the orthogonal space. But what if instead of just $b$, there where some time dependance, eg. $b + F(t)$, or (more illustrative)
\begin{equation*}
\begin{aligned}
\dot{u}(t) &= Au(t) + b + F(t)
\end{aligned}
\end{equation*} 
In this case the projection method needs to be used two times, one time to solve $ \dot{u}(t) = Au + b $ and another to solve $ \dot{u}(t) = Au + F(t) $. The solutions can then be added together to solve the original problem. \\
The reason for this is the need for a vector that can generate the orthogonal space. In this case there is no common vector $\tilde{b}$ so that $\tilde{b} \tilde{F}(t) = b + F(t)$. \\


An even bigger problem arises when a differential equation has a source term that is not separable in time and space, read more about the reason for this in \cite{elena} and \citep{min}. An equation that is separable in time and space can be written as $p(t,x,y) = g(x,y) \cdot F(t)$. If $p(t,x,y)$ is not separable, this is impossible. As an example, consider the wave equation
\begin{equation*}
\begin{aligned}
\frac{\partial^2 q(t,x,y)}{\partial t^2} = \frac{\partial^2 q(t,x,y)}{\partial x^2 } \frac{\partial^2 q(t,x,y)}{\partial y^2 } + g(t,x,y) \quad \text{ where } g(t,x,y) \neq p(x,y) F(t) \\
\end{aligned}
\end{equation*}
This equation cannot be discretized with a single vector $b$. If this is discretized to be on the form of \eqref{eqn:PDE} and solved with a projection method, it would look like:
\begin{equation*} \label{eqn:terrible}
\dot{u}(t) = A u(t) + \sum \limits_{i = 1}^{\hat{m}} e_i F_i(t),
\end{equation*}
where $\hat{m}$ is the size of the matrix, and $F_i(t)$ is a time dependent function after spacial discretization. The reason for this is that every different point, $x_i$ and $y_i$, gives a different time dependent function, and all of these functions need to be used to obtain the correct solution. \\
The reason the canonical vectors $e_i$ are used is for simplicity, any set of orthogonal vectors can be used. \\
This means that equation \eqref{eqn:terrible} needs to be solved $\hat{m}$ times to obtain the solution. This gives the projection methods a terrible run time. It is not advised  to use any projection method if this is the case \cite{min}.
\newpage


\subsection{Number of operations}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Since computation times might be uncertain due to different additional loads (web surfing, writing and so on) there will here be a short comparison between the number of operations for each method. This will also give a basis for what computationally complexities to expect from the different methods.\\

\begin{table}
\begin{tabular}{l l }
Operation & Cost \\
\hline
Integration with forward Euler & $\mathcal{O}(k n^2)$ \\
Integration with Trapezoidal or midpoint rule & $\mathcal{O}(k n^3)$ \\
Arnoldi's algorithm & $ \mathcal{ O }(n^2 \hat{m})$ \\
Symplectic Lanczos method & $ \mathcal{O}(n^2 \hat{m}) $\\
Transforming from $z_n(t)$ to $u_n(t)$ & $ \mathcal{O}(\hat{m}nk) $\\
Matrix vector multiplication ($\hat{m}\times n$)(sparse matrix) & $ \mathcal{O}(\hat{m}) $ \\
Matrix vector multiplication ($\hat{m}\times n$) (dense matrix) & $ \mathcal{O}(n \hat{m}^2) $
\end{tabular}

\caption{Computational cost of some mathematical operations. $n$ is restart variable, $\hat{m}$ is the size of the large matrix, and $k$ is the number of steps in time. Numbers from this table is obtained from \cite{complex}. }
\label{tab:cd}
\end{table}

A table of computational cost for different operations is given in table \ref{tab:cd}. \\
Surprisingly the asymptotic cost for \texttt{KPM} and \texttt{SLM} is the same. A lot of small additional costs for \texttt{SLM} is hidden here meaning that \texttt{KPM} probably has a smaller computation time, but the computation times are increasing similarly. The windowing is also a wild card a since lot of costly operations are hidden. The difference between the projection methods and \texttt{DM} strongly depends on $n$ and $r_n$, this makes it impossible to conclude anything without actually doing it, except that all methods are linear with $k$. \\

\begin{table}
\begin{tabular}{l | l}
Method & Explonation and number of operation \\
\hline
\texttt{KPM} & Arnoldi's algorithm, an integration method, and the transformation.
\\ & $ \mathcal{O}((n^2 \hat{m} + k n^3 + \hat{m}nk)i_r)$ \\ 
\texttt{SLM} & Symplectic Lanczos method, an integration method and the transformation. 
\\ & $ \mathcal{O}((n \hat{m}^2 + k n^3 + \hat{m}nk)i_r) $  \\
\texttt{DM} & An integration method with $n = \hat{m}$. 
\\  & $\mathcal{O}(k\hat{m}^3)$ \\
Windowing  & \texttt{SLM} or \texttt{KPM} needs to be run $K$ times. \\ (\texttt{KPM} or \texttt{SLM}) & $\mathcal{O}((n \hat{m}^2 + k n^3 + \hat{m}nk)i_r K)$ \\
\end{tabular}
\label{tab:cc}
\caption{ Number of operations needed for the different methods with trapezoidal rule. $i_r$ is the number of restarts needed for the method to converge. For windowing $K\cdot k$ is equal to $k$ for the other methods. Windowing for \texttt{DM} is not interesting since it has exactly the same run time, and is exactly the same method. }
\end{table}

\section{\texttt{SLM} and its eigenvalue solving properties} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
If you do a quick internet search of "symplectic Lanczos method" most articles you find are in relation to symplectic Lanczos method's ability to approximate eigenvalues of Hamiltonian matrices. This section I will try to explain what makes this algorithm so attractive for these types of problems. (This section is based on reading \cite{SLM1},\cite{SLM2}, \cite{SLM3}, and \cite{SLM4}.) \\

The eigenvalue problem is found in many different areas, eg. control theory, model reduction, system analysis. No other algorithm that exploits the Hamiltonian structure and the sparsity of the matrices occurring in these areas. \\

\texttt{SLM} is a relatively fast algorithm for transforming big sparse Hamiltonian matrix to smaller sparse Hamiltonian matrix with similar properties. Eigenvalues of Hamiltonian matrices comes in pares, $\{ \pm \lambda \} $, or in quadruples, $\{ \pm \lambda $, $\pm \bar{\lambda} \} $. Since the small matrix is Hamiltonian, it is easier to find these pairs or quadruples. An other important property is that the biggest, and therefore often most important eigenvalue, are found first.\\

The method is not without is flaws. Frequent breakdown due to ill conditioned matrices occur. But due to its large potential much work has been make to overcome the difficulties. The most promising improvements are the different types of restarts. This way the numerical estimations can be improved without loosing the Hamiltonian properties, and without a to high cost.
Unfortunately not all are very efficient. Though fear not, there is still much ongoing research on the topic. \\

An other method based on \texttt{SLM} is the \texttt{SR} - algorithm which has similarities with the \texttt{QR} - algorithm. \texttt{SR} applies symplectic operations to a Hamiltonian matrix instead of applying orthogonal operations as in \texttt{QR}. \\

Not all the papers are interested in improving the method, \cite{SLM4} shows under which assumptions the method converges. \\

Interestingly many of these papers also compare \texttt{SLM} to \texttt{KPM}.


 
