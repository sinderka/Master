%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This chapter will contain the theoretical aspects of the projection methods, such as derivation, proof of convergence and assumptions. 
 
\section{Zero initial condition}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:inittransf}
For both \texttt{KPM} and \texttt{SLM} it is important that the initial conditions are zero. The reason for this is explained in section \ref{sec:solmet}. Equation \eqref{eqn:PDE} can be transformed so that it has zero initial conditions in the following way \cite{zerotransf}: \\
Start by shifting the solution
\begin{equation*}
\hat{u}(t) = u(t)-u_0,
\end{equation*}
then rewrite the original equation as
\begin{equation}
\begin{aligned}
\dot{\hat{u}}(t) &= A \hat{u}(t) +A u_0\\
 \hat{u}(0)&= 0. \\
\end{aligned}
\label{eqn:shiftedproblem}
\end{equation}
\noindent The equation above solves the shifted problem. The original problem can be solved by shifting it back with
\begin{equation*}
 u(t) = \hat{u} + u_0. \\
\end{equation*}


\noindent All test problems with a non-zero initial condition will be transformed in this way without using the hat notation. The letter $b$ will be used as a collective term to describe the product $A u_0$, or any constant vector occurring on the right hand side of equation \eqref{eqn:PDE}.

\section{Energy}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
It is well known that the energy of a system on the form of equation \eqref{eqn:PDE} can be expressed as \cite{energy}
\begin{equation*}
\begin{aligned}
\mathcal{H}_1(u) = \frac{1}{2} u^\top (t) J A u(t)
\end{aligned}
\end{equation*}
\noindent If the transformation in section \ref{sec:inittransf} is used, the energy is 
\begin{equation}
\mathcal{H}_2 (\hat{u}) = \frac{1}{2} \hat{u}^\top (t)  J A \hat{u}(t) + \hat{u}^\top (t)  J b.
\label{eqn:energy2}
\end{equation}
\noindent In theoretical derivations $\mathcal{H}_2$ will be used, as the shifted problem is the one actually solved. In figures however, $\mathcal{H}_1$ is used since the unshifted solution is what is sought. \\

\noindent There will be a discussion about different types of test problems. The ones marked "constant energy" will be problems on the form of equation \eqref{eqn:PDE}. The other type is problems marked "varying energy", which is on the form of equation \eqref{eqn:PDE1}. Test problems will be presented in section \ref{sec:testprob}.

\section{Integration methods}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent The time domain $[0,T_s]$ will be divided in $k$ pieces, so that the step size is $h_t = T_s/k$. The time discretized solution of $u(t_j)$ will be called $U_j$, with $t_j = j h_t$ where $ j = 1,2,\dots,k $. Since the initial value is known to be zero, $j = 0$ is disregarded. Let the discretization of $f(t_j)$ be called $F_j$. The integration methods considered in this text are trapezoidal rule, forward Euler, and midpoint rule. The definition and the iteration scheme of the different methods are given in table \ref{tab:intmet}. \\

\begin{table}

\caption{Methods for integrating in time. Note that since the midpoint rule uses the midpoint $F_{i+\frac{1}{2}}$, twice as many points needs to be saved for midpoint rule as for the other methods. Trapezoidal and midpoint rule have quadratic convergence rates, while forward Euler has linear convergence. To compare the methods it is therefore necessary to use the squared number of points for forward Euler as for the other methods. $g$ is the right hand side of equation \eqref{eqn:PDE} or \eqref{eqn:PDE1}.}
\centering
\begin{tabular}{l l}
	Trapezoidal rule (\texttt{trap}) \cite{trapezoidal} & $ \frac{U_{i+1} - U_{i}}{h_t} = g \Big( \frac{1}{2}(t_i+t_{i+1}),\frac{1}{2}(U_i+U_{i+1}) \Big)$
	\\ & $U_{i+1} = (I- \frac{A h_t}{2}) ^{-1} \Big(  U_i + \frac{h_t}{2} \big( A U_i+(F_{i+1}+F_i) \big)  \Big) $\\
\hline	
	Forward Euler (\texttt{Euler}) \cite{forwardeuler} & $ \frac{U_{i+1} - U_{i}}{h_t} = g ( t_i, U_i ) $ \\ & $ U_{i+1} = U_i + h_t \big( A U_i + F_i \big) $ \\
	\hline
	Midpoint rule (\texttt{mid}) \cite{midpoint} & $ \frac{U_{i+1} - U_{i}}{h_t} =  g \Big(  t_{i+\frac{1}{2}} , \frac{1}{2}(U_i + U_{i+1})    \Big) $ \\ & 
	$U_{i+\frac{1}{2}} = U_i + \frac{h_t}{2} ( A U_i + F_{\frac{1}{2}} )$ \\ &
    $U_{i+1} = (I-\frac{A h_t}{2}) ^{-1} (U_{i+\frac{1}{2}} + \frac{h_t}{2} F_{i+ \frac{1}{2}})$
    
    
\end{tabular}


\label{tab:intmet}
\end{table}
\noindent Trapezoidal rule and midpoint rule are the same methods if the energy is constant. When $F_j$  is not constant, midpoint rule should have an advantage because it is symplectic \cite{symplecticintegrator}. \\

\noindent Forward Euler has no energy preserving properties, and is used to show the difference between a naive integration method, and energy preserving integration methods. \\

\noindent In addition to the iteration schemes in table \ref{tab:intmet}, some exact solvers are used. They are presented in table \ref{tab:intcorrect}.
\begin{table}

\caption{Methods for exact integration in time. Since they are very computationally demanding they will only be used on small projected matrices. They also need the test problem to have constant energy. The expected convergence will be depending on the approximation of $A$, since this method is only exact in time. These function are explained in matlab's docmentation: \cite{expm}. }

\begin{tabular}{l l}
Eigenvalue and diagonalization (\texttt{diag}) & $[V,D] = \texttt{eig}(A)$ \\
 & $U_i = V \cdot \texttt{diag} \Big( \texttt{exp} \big( \texttt{diag}(D \cdot t_i)\big)\Big)/V \cdot b - b$ \\
Matlab's \texttt{expm} function (\texttt{expm}) & $U_i = \texttt{expm}(A \cdot t_i) \cdot b - b$ \\

\end{tabular}

\label{tab:intcorrect} 
\end{table}

\subsection{Energy conservation for trapezoidal rule} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This section will show the energy preserving properties of trapezoidal rule on the initial value shifted function
\begin{equation}
\begin{aligned}
\dot{u}(t)& = Au + b \\
u(0)& = 0.
\end{aligned}
\label{eqn:energyconvinit}
\end{equation}
\noindent Note that $b$ can be any constant vector, not just $A u_0$. The proof is based on \cite{convtrap}.
The energy of this function has already been presented in equation \eqref{eqn:energy2}. The main ingredients in this proof are the gradient of $\mathcal{H}_1$ and the iterations scheme for the trapezoidal rule. Assume that $A$ is a Hamiltonian matrix, so that $JA$ i symmetric.\\ 
The gradient of $\mathcal{H}_1(u)$ is 
\begin{equation*}
\begin{aligned}
\nabla \mathcal{H}_1(u) = J (Au + b) .
\end{aligned}
\end{equation*}
\noindent The trapezoidal rule found in table \ref{tab:intmet}, used on equation \eqref{eqn:energyconvinit} gives 
\begin{equation*}
\begin{aligned}
\frac{U_{j+1} - U_j}{h_t} = A \frac{U_{j+1}  + U_j}{2} + b.
\end{aligned}
\end{equation*}
\noindent Substituting $\frac{U_{j+1}  + U_j}{2} $ for $u$ gives the gradient of the energy for this function:
\begin{equation*}
\begin{aligned}
\nabla \mathcal{H}_1 Big(\frac{U_{j+1}  + U_j}{2}\Big) = JA \frac{U_{j+1}  + U_j}{2} + J b.
\end{aligned}
\end{equation*}
\noindent Since
%\begin{equation*}
$\frac{ U_{j+1} - U_j}{h_t} = J^{-1} \nabla \mathcal{H}_1( \frac{U_{j+1}  + U_j}{2} ) $
%\end{equation*} 
\noindent and
%\begin{equation*}
$\nabla \mathcal{H}_1(\frac{U_{j+1}  + U_j}{2})^\top J^{-1} \nabla \mathcal{H}_1(\frac{U_{j+1}  + U_j}{2}) = 0$,
%\end{equation*}
\noindent we have
\begin{equation*}
\begin{aligned}
\nabla \mathcal{H}_1 (\frac{U_{j+1}  + U_j}{2}) ^\top \frac{U_{j+1} - U_j}{ h_t } = 0.
\end{aligned}
\end{equation*}
\noindent Substituting $ J (A\frac{U_{j+1}  + U_j}{2}  + b) $ for $\nabla \mathcal{H}_1 (\frac{U_{j+1}  + U_j}{2})$ and rewriting gives
\begin{equation*}
\begin{aligned}
\mathcal{H}_1(U_{j+1}) - \mathcal{H}_1(U_{j}) = 0.
\end{aligned}
\end{equation*}
\noindent Hence trapezoidal rule conserves the energy for functions with constant energy. Since the initial conditions are satisfied the energy will have the correct value at all times.

\section{Windowing}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent The projection methods has a tendency to stop working when the time domain is too large. A fix to this might be to divide the time domain in smaller pieces, and solve each piece individually. How this is done is described in the next paragraph.\\ 

\noindent Let $T_s$ denote simulated time, $K$ be the number of subintervals $T_s$ is divide into, and $k$ be the number of pieces each the $K$ is divided into. Each of the $K$ subintervals is then solved as separate problems, with the initial conditions updated. This is explained in a more precise manner in algorithm \ref{alg:Kversusk}. This method will be called "windowing", as this is the name my supervisor Elena Celledoni \cite{elenaperson} used to describe the method.

\begin{algorithm} [h!]
\begin{algorithmic} \caption{ Windowing } \label{alg:Kversusk}  
\STATE Start with an initial value $U_0 \in \mathbb{R}^{\hat{m}}$, $K \in \mathbb{N}$ and $k \in \mathbb{N}$.
\STATE Make an empty vector $U$.
\FOR{$j = 1,2,\cdots, K $} 
   \STATE Solve differential equation with $k+1$ points in time and initial value $U_0$.
   \STATE Place the new points at the end of $U$.
   \STATE Update $U_0$ to be the last value of $U$.
   \STATE Delete the last point of $U$.
\ENDFOR
\STATE Return $U$.
\end{algorithmic} 
\end{algorithm}

\section{Solution methods} \label{sec:solmet} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent There are two orthogonalisation methods that will be discussed in this text. Their names are symplectic Lanczos method (\texttt{SLM}), and Arnoldi's algorithm (\texttt{KPM}, as it is implemented as the Krylov projection method). The derivation of the methods, together with a proof of energy perseverance for \texttt{SLM}, is presented in this section. \\


\noindent Projection methods are ways to make approximated solutions from a subset of the original problem by projecting a problem of several dimensions onto a smaller dimensional problem. One great feature of these methods is that a smaller system of equations can be used to obtain solutions, making further computations less demanding. However finding this projected system can be time consuming. Another drawback is that the projected solutions are approximations. The approximated solution can be improved by something called restarting, meaning using the projection method again, to solve an equation for the difference between the projected solution and the unprojected solution. This can be done repeatedly to obtain the desired accuracy. \\

\noindent Assume that the equations are on the form
\begin{equation}
\begin{aligned}
\dot{u}(t) &= Au(t) + b \\
u(0) &= 0
\end{aligned}
\label{eqn:PMform}
\end{equation}
\noindent when these methods are used. Note that the initial values are zero otherwise it is difficult to know how well the initial value is approximated. It is also important that $A$ is a Hamiltonian matrix when using \texttt{SLM}. \\

\noindent Note that the relation between $\hat{m}$, $\tilde{m}$ and $m$ used in the algorithms is given by $\hat{m} = 2\tilde{m}= 2(m-2)^2$ and that $ n = 2\tilde{n}$. $\hat{m}$ is the dimension of the original system and $n$ is the size of the orthogonal system. $n$ is called the restart variable. Don't worry to much about these details, it is just a way to simplify the expressions. The reason to simplify them this way will be described in section \ref{sec:testprob}. \\

\noindent When talking about the true solution, it is the meant as the unprojected solution (the same problem, solved exactly the same way, except without any projection method), and will later be referred to as the direct method(\texttt{DM}).
\subsection{Arnoldi's Algorithm and the Krylov projection method} %%%%%%%%%%%%%%%%%
This section is loosely based around the derivation of the method done in \cite{elena} and \cite{min}. \\

\noindent The Krylov subspace is the space $W_n (A,b) = \{b,Ab, \cdots, A^{n-1}b\} = \{v_1,v_2,\cdots,v_n\} $, where $n \leq \hat{m}$.
The vectors $v_i$ together with $h_{i,j} = v_i^\top Av_j$ are found by Arnoldi's algorithm, shown in algorithm \ref{alg:arnoldi}. Let $V_n$ be the $\hat{m} \times n$ matrix consisting of column vectors $[v_1,v_2,\cdots,v_n ] $ and $H_n$ be the $n \times n$ upper Hessenberg matrix containing all elements $(h_{i,j})_{i,j=1,\cdots,n}$. Then the following holds \cite{kryprop}:

\begin{equation}
\begin{aligned}
AV_n & = V_n H_n + h_{n+1,n}v_{n+1}e^\top_n  \\
V^{\top}_n AV_n &= H_n  \\
V_n^{\top} V_n &= I_n. 
\label{eqn:propA}
\end{aligned}
\end{equation}

\begin{algorithm} [h!]
\begin{algorithmic} \caption{Arnoldi's algorithm\cite{arnold}} \label{alg:arnoldi}  
\STATE Start with $A \in \mathbb{R}^{\hat{m} \times \hat{m}}$, $b \in \mathbb{R}^{\hat{m}}$, $n \in \mathbb{N}$ and a tolerance $\iota \in \mathbb{R}$.
\STATE $v_1 = b/\|b \|_2$
\FOR{$j = 1,2,\cdots, n $} 
   \STATE Compute $h_{i,j} =  v_i^{\top}Av_j,v_i $ for $i = 1,2,\dots, j$
    \STATE Compute $w_j = A v_j - \Sigma_{i=1}^{j} h_{i,j}v_i $
    \STATE $h_{j+1,j} = \| w_j \|_2$
    \IF{$h_{j+1,j} < \iota $} %
        \STATE\textbf{STOP}
    \ENDIF 
   \STATE $v_{j+1} = w_j/h_{j+1,j}$
\ENDFOR
\STATE Return $H_n$, $V_n$, $v_{n+1}$, $h_{n+1,n}$.
\end{algorithmic} 
\end{algorithm}



\noindent Here, $e_n$ is the $n$th canonical vector in $\mathbb{R}^n$. $n$ is the number of iterations performed by Arnoldi.\\

\noindent By using the transformation $u(t) \approx V_n z_n(t)$, equation \eqref{eqn:PMform} can, with the help of equation \eqref{eqn:propA}, be written as

\begin{equation}
\begin{aligned}
\dot{z}_n(t) &= H_n z_n(t) + \| b\|_2 e_1 f(t)  \\
z_n(0) &= 0.
\label{eqn:KPMi}
\end{aligned}
\end{equation}
The approximated solution of the original problem can be attained by the relation
\begin{equation*}
u_n(t) = V_n z_n(t),
\end{equation*}
where $u_n$ is the approximated solution found with $n$ as a restart variable. \\

\noindent Convergence for the method can be shown by finding an expression for the residual, which can be written as
\begin{equation*}
r_n(t) = v f(t) -\dot{u}_n(t) + A u_n(t).
\end{equation*}
This can be rewritten with $u_n(t) = V_n z_n(t)$, to get
\begin{equation*}
r_n(t) = v f(t) - V_n \dot{z}_n(t) + A V_n z_n(t).
\end{equation*}
By equation \eqref{eqn:propA}, it can be simplified to
\begin{equation}
r_n(t) = h_{n+1,n} e_n^\top z_n(t) v_{n+1}.
\label{eqn:Aresidual}
\end{equation}
\noindent Since $h_{n+1,n}$ is zero for some $n \leq \hat{m}$ ($V_{\hat{m}} h_{n+1,n} v_{n+1} = 0 $ by construction), the procedure will converge toward the correct solution $u(t)$. \\

\noindent Larger $n$ gives a better approximation of the solution, but also higher computational complexity. The drawback is also that there is no way of knowing how well the solution is approximated in advance. The size of $h_{n+1,n}$ does say something about how well the solution is approximated (smaller $h_{n+1,n}$ means a smaller error), though it is only available after solving the problem. If the approximation is not sufficient the solution must be recalculated with larger $n$, unless you perform a restart. \\

\noindent A restart considers the difference $\epsilon_n^{(i)}(t) = u(t)-u_n^{(i)}$ given by:
\begin{equation}
\begin{aligned}
\dot{\epsilon}_n^{(i)}(t)=A \epsilon_n^{(i)} (t)  - r_n^{(i)},
\end{aligned}
\label{eqn:KPMdiff}
\end{equation}
where 
\begin{equation*}
r_n^{(i)} = h_{n+1,n}^{(i-1)} v_{n+1}^{(i-1)} e_n^{\top} \epsilon_n^{(i-1)} (t).
\end{equation*}
\noindent The expression for $r_n^{(i)}$ is exactly the same as in equation \eqref{eqn:Aresidual}, except that $z_n(t)$ is replaced with $\epsilon_n(t)$, and the counter $i$ is present. $u_n^{(i)}$ is the solution obtained after $i$ restarts with $n$ as a restart variable. 
\noindent Equation \eqref{eqn:KPMdiff} can be simplified by using equation \eqref{eqn:propA}, and writing $ \epsilon^{(i)}_n(t)  = V_n \delta_n^{(i)}(t) $, to obtain the following:
\begin{equation}
\dot{\delta}^{(i)}_n(t) = H_n^{(i)} \delta_n^{(i)}(t) + e_1 h_{n+1,n}^{(i-1)} e^\top_n \delta_n^{(i-1)}(t), \quad i \geq 1.
\label{eqn:KPMr}
\end{equation}
\noindent The solution is found by $ u_n^{(i)}(t) = \sum \limits_{j = 0} ^i V_n^{(j)} \delta_n^{(j)} (t) $, where $\delta_n^{(0)} (t) = z_n(t)$ and found by equation \eqref{eqn:KPMi}. $H_n$, $v_{n+1}$, $h_{n+1,n}$ and $V_n$ without counting variables will always be used together with equation \eqref{eqn:KPMi}. \\
Repeatedly solving this equation can increase the accuracy of the approximated solution within an arbitrary constant of the desired solution. It is no longer possible to use $h_{n+1,n}$ as a measure for the error when the restart is used, since Arnoldi's algorithm has no way to measure how much this iteration improved the solution compared to previous iterations.\\

\noindent The proof of convergence for the restart can be found in \cite{elenaconv}.\\
\subsection{Symplectic Lanczos method}
\texttt{SLM} and \texttt{KPM} are very similar, which is easy to see when comparing equation \eqref{eqn:propA} and \eqref{eqn:propS}. The main difference is that orthonormality in Arnoldi is replaced by symplecticity in \texttt{SLM}. This makes the derivation quite similar.\\

\noindent Let $S_n = [v_1,v_2,\dots v_{\frac{n}{2}},w_1,w_2,\dots w_{\frac{n}{2}}]$ be a set of $J$-orthogonal vectors satisfying the following equations \cite{SLMprop}:
\begin{equation}
\begin{aligned}
AS_n &= S_n H_n + \zeta_{n+1} v_{n+1} e_{\hat{m}}^\top\\
J_{n}^{-1} S_n^\top J_{\hat{m}} A S_n &= H_n \\
S_n^{\top} J_{\hat{m}} S_n &= J_{n}.\\
\label{eqn:propS}
\end{aligned}
\end{equation}
\begin{algorithm} \caption{Symplectic Lanczos method \cite{SLM}, with reortogonalization from \cite{SLMO}. } \label{alg:symlanz}
\begin{algorithmic}
\STATE Start with a Hamiltonian matrix $A \in \mathbb{R}^{2\tilde{m} \times 2 \tilde{m}}$, $b \in \mathbb{R}^{2 \tilde{m}}$, $\tilde{n} \in \mathbb{N}$
\STATE $v_0= 0 \in \mathbb{R}^{2 \tilde{m}}$
\STATE $\zeta_1 = \| b\|_2$
\STATE $v_1= \frac{1}{\zeta_1}  b$
\FOR {$j = 1,2, \dots, \tilde{n}$}
	\STATE $v = A v_j$
	\STATE $\delta_j =  v_j^\top v$
	\STATE $\tilde{w} = v-\delta_j v_j$
	\STATE $\kappa_j = v_j^\top J_{\tilde{m}} v $
	\STATE $w_j = \frac{1}{\kappa_j} \tilde{w_j}$
	\STATE $w = A w^j$
	\STATE $ \tilde{S}_{j-1} = [v_1,v_2,\dots,v_{j-1},w_1,w_2,\dots,w_{j-1}] $
	\STATE $ w_j = w_j + \tilde{S}_{j-1}J_{j-1} \tilde{S}_{j-1}^\top J_{\tilde{m}} w_j $
	\STATE $\beta = -w_j^\top J_{\tilde{m}} w$
	\STATE $\tilde{v}_{j+1} = w - \zeta_j v_{j-1} - \beta_j v_j + \delta_j v_j$
	\STATE $ \zeta_{j+1} = \|\tilde{v}_{j+1} \|_2 $
	\STATE $ v_{j+1} = \frac{1}{\zeta_{j+1}} \tilde{v}_{j+1} $
	\STATE $ \tilde{S}_j = [v_1,v_2,\dots,v_{j},w_1,w_2,\dots,w_{j}] $
	\STATE $ v_{j+1} = v_{j+1} + \tilde{S}_j J_j \tilde{S}_j^\top J_{\tilde{m}} v_{j+1} $
\ENDFOR
\STATE $S_n = [v_1,v_2,\dots,v_{\tilde{n}},w_1,w_2,\dots,w_{\tilde{n}}]$
\STATE $H_n = \begin{bmatrix}
\text{diag} \big( [\delta_j]^{\tilde{n}}_{j=1} \big) & \text{tridiag}\big( [\zeta_j]_{j=2}^{\tilde{n}},[\beta_j]_{j=1}^{\tilde{n}},[\zeta_j]_{j=2}^{\tilde{n}} \big) \\
\text{diag} \big( [\kappa_j]^{\tilde{n}}_{j=1} \big) & \text{diag} \big( [-\delta_j]^{\tilde{n}}_{j=1} \big)
\end{bmatrix} $
\STATE Return $H_n$, $S_n$, $v_{n+1}$, $\zeta_{n+1}$.
\end{algorithmic}
\end{algorithm}


\noindent Here $H_n$ is an $ n \times n $ matrix. $ \tilde{n} = \frac{n}{2}$ is the number of iterations the algorithm performed, since the algorithm makes two vectors per iterations: $v_j$ and $w_j$. \\

\noindent The process of making the vectors and the matrix is a little more involved than for Arnoldi's algorithm, so there will be no thorough explanation of how it works, except for in Algorithm \ref{alg:symlanz}.\\

\noindent Transform the problem in equation \eqref{eqn:PMform} with $u(t) \approx S_n z_n(t)$, multiply with $J^{-1}_n S_n^\top J_{ \hat{m} }$, and simplify with equation \eqref{eqn:propS} to obtain
\begin{equation*}
\begin{aligned}
\dot{z}(t) = H_n z_n(t) + J_n^{-1} S_n^\top J_{\hat{m}} b f(t).
\end{aligned}
\end{equation*}
This can be simplified when writing $ b = S_n e_1 \| b \|_2 $ and using equation \eqref{eqn:propS}:
\begin{equation}
\begin{aligned}
\dot{z}(t) = H_n z_n(t) + \|b \|_2 e_1 f(t).
\end{aligned}
\label{eqn:SLMi}
\end{equation}
Equation \eqref{eqn:SLMi} and \eqref{eqn:KPMi} are identical, except for the underlying assumptions. \\

\noindent Since $ S_{\hat{m}}^\top J_{\hat{m}} \zeta_{n+1} v_{n+1} = 0 $ by construction \cite{SLMconv}, the proof of convergence is very similar to the proof for \texttt{KPM}.\\
The residual of the method can be written as
\begin{equation*}
r_n(t) = v f(t) - \dot{u}_n(t) A u_n(t).
\end{equation*}
This can rewritten with $u_n = S_n z_n(t)$.
\begin{equation*}
r_n(t) = v f(t) -S_n \dot{z}_n(t) + A S_n z_n(t).
\end{equation*}
By equation \eqref{eqn:propS} it becomes
\begin{equation*}
r_n(t) =  \zeta_{n+1} v_{n+1} e_{\hat{m}}^\top z(t).
\end{equation*}
Since $\zeta_{n+1}$ will approach zero as $n$ grows, $r_n$ will approach zero, and the method will converge. \\

\noindent A restart can also here be performed if the accuracy of the solution is not satisfactory. This can be derived by looking at the difference $ \epsilon_n = u(t) - S_n z_n(t)$: %The result is given in equation \eqref{eqn:SLMr}. 
\begin{equation}
\dot{\epsilon}_n^{(i)}(t) = A \epsilon_n^{(i)}(t) + r_n^{(i)}(t),
\label{eqn:resenerg}
\end{equation}
where
\begin{equation*}
r_n^{(i)}(t) = \zeta_{n+1}^{(i-1)} v_{n+1}^{(i-1)} e_{\hat{m}}^\top \epsilon_n^{(i-1)}(t).
\end{equation*}

\noindent Write $ \epsilon^{(i)}_n(t)  = V_n \delta_n^{(i)}(t) $ to obtain
\begin{equation*}
\begin{aligned}
\dot{\delta}_n^{(i)} = H_n^{(i)} \delta_n^{(i)} + J^{-1}_n {S_n^{(i)}}^\top J_{\hat{m}} \zeta_{n+1}^{(i-1)}v_{n+1}^{(i-1)} e_n^\top \delta_n^{(i-1)}.
\end{aligned}
\end{equation*}

\noindent This can be further simplified to
\begin{equation}
\begin{aligned}
\dot{\delta}_n^{(i)} = H_n^{(i)} \delta_n^{(i)} + e_1 \zeta_{n+1}^{(i-1)} e_n^\top \delta_n^{(i-1)}, \quad i \geq 1.
\label{eqn:SLMr}
\end{aligned}
\end{equation}
\noindent The solution is found by $ u_n^{(i)}(t) = \sum \limits_{j = 0} ^i S_n^{(j)} \delta_n^{(j)} (t) $, where $\delta_n^{(0)} (t) = z_n(t)$ and found by equation \eqref{eqn:SLMi}. $H_n$, $v_{n+1}$, $\zeta_{n+1}$ and $S_n$ without counting variables refers to equation \eqref{eqn:SLMi}. \\

\noindent Proof of convergence and other interesting results for this method can be found in \cite{SLMinteresting}. 

\subsubsection{Proof that \texttt{SLM} without restart is energy preserving} %%%%%%%%%%%%%%%%%%%%%%
If equation \eqref{eqn:SLMi} is solved by an energy preserving method, eg. trapezoidal rule, the energy will be preserved \cite{SLMpreserve}. The energy of this equation is
\begin{equation*}
\mathcal{H}(z_n) = \frac{1}{2}z_n(t)^\top J_n H_n z_n(t) + z_n(t)^\top J_n e_1 \|b \|_2
\end{equation*}

\noindent While the energy of the original problem is 
\begin{equation*}
\mathcal{H}(u_n) = \frac{1}{2}u_n(t)^\top J A u_n(t) + u_n(t)^\top J b.
\end{equation*}
Perform the substitution $ u_n(t) = S_n z_n(t) $ to get
\begin{equation*}
\mathcal{H}(u_n) = \frac{1}{2}z_n(t)^\top S_n^\top J A S_n z_n(t) + z_n(t)^\top S_n^\top J b.
\end{equation*}
Use that $ b = S_n e_1 \| b \|_2 $, and simplify with equation \eqref{eqn:propS}:
\begin{equation*}
\mathcal{H}(u_n) = \frac{1}{2}z_n(t)^\top J_n H_n z_n(t) + z_n(t)^\top J_n e_1 \|b \|_2.
\end{equation*}
This results in: 
\begin{equation*}
\mathcal{H}(z_n) - \mathcal{H}(u_n) = 0.
\end{equation*}
Since the transformation does not change the energy, \texttt{SLM} is energy preserving. This will not hold for \texttt{KPM} for a couple of reasons. First, $H_n$ is not a Hamiltonian matrix, so the method itself is not energy preserving. The other reason is that the transformation $V_n$ is not symplectic.\\ 
\texttt{SLM} with restart is also not energy preserving since the error equation is depending on time, which ruins the energy preserving properties. %But this property can be somewhat regain by restarting several times.
\subsubsection{Residual energy}
Equation \eqref{eqn:energy3} and \eqref{eqn:energy4} are solely used to describe the residual energy of \texttt{SLM}. $\mathcal{H}_3$ describes the residual energy of the projected method in $u_n$ space, that is, after transforming back from $z_n$ space. $\mathcal{H}_4$ is the residual energy in $z_n$ space. In theory these energies should be equal and zero. Section \ref{sec:residualenergy} shows how this holds up in practice. 
They are presented in the equation below, and can easily be derived from \eqref{eqn:resenerg} and \eqref{eqn:SLMr}. \\
\begin{equation}
\mathcal{H}_3 (t) = \frac{1}{2} {\epsilon^{(1)}}^\top (t) J_m A \epsilon^{(1)} + {\epsilon^{(1)}}^\top J_m h_{n+1,n}^{(1)} v_{n+1}^{(1)} e_n^\top z(t)
\label{eqn:energy3}
\end{equation}

\begin{equation}
\mathcal{H}_4 (t) = \frac{1}{2} {\delta^{(1)}}^\top (t) J_n H_n^{(2)} \delta^{(1)} + {\delta^{(1)}}^\top {S_n^{(2)}}^\top  J_m h_{n+1,n}^{(1)} v_{n+1}^{(1)} e_n^\top z(t)
\label{eqn:energy4}
\end{equation}
The iteration variables are present since it considers the residual with one restart.

\subsection{A comment on the orhogonalisation methods} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The  derivation of the methods above shows the many similarities between \texttt{SLM} and \texttt{KPM}. The only practical difference between the two methods is the orthogonalisation methods. This makes implementation of the methods very similar.  \\

\noindent How to obtain the correct number of restarts to get a good estimate of the solution will be discussed in section \ref{sec:praktisk} and \ref{sec:resultat}.

\subsection{Direct method} \label{sec:DM}
It is important to have some method to compare these projection methods with. This will be done by solving the problem exactly the same way, but without using any projection method. This will be called direct method, or \texttt{DM}. \\

\noindent The energy of \texttt{DM} will be constant if an energy preserving method is used, while the error will increase linearly with $T_s$ \cite{linearerrorgrowth}. A goal of this text is to see how \texttt{KPM} and \texttt{SLM}'s error and energy behaves compared to this. 

\noindent The proof that \texttt{DM} is the natural method to compare with will be shown here with trapezoidal rule for \texttt{KPM}. The proof is very similar for \texttt{SLM}. \\

\noindent We start by stating the desired equation discretized with the trapezoidal rule:
\begin{equation*}
\begin{aligned}
\Big(I-\frac{Ah_t}{2} \Big) U_{i+1} = \Big( U_i + \frac{h_t}{2} \big( A U_i + F_{i+1} +F_{i} \big) \Big.
\end{aligned}
\end{equation*}
Applying the transformation $ U_i = V_n Z_i(t) $ gives
\begin{equation*}
\begin{aligned}
\Big(V_n-\frac{A V_n h_t}{2}\Big) Z_{i+1} = \Big( V_n Z_i + \frac{h_t}{2} \big( A V_n Z_i + F_{i+1} +F_{i} \big) \Big).
\end{aligned}
\end{equation*}
Using equation \eqref{eqn:propA} gives
\begin{equation*}
\begin{aligned}
\Big(V_n-h_t\frac{V_n H_n + h_{n+1,n}v_{n+1} e_n^\top }{2}\Big) Z_{i+1} = \Big( V_n Z_i + \frac{h_t}{2} \big( (V_n H_n + h_{n+1,n}v_{n+1} e_n^\top) Z_i + F_{i+1} +F_{i} \big) \Big).
\end{aligned}
\end{equation*}
When using the trapezoidal rule directly on \eqref{eqn:KPMi}, the result is
\begin{equation*}
\begin{aligned}
\Big(V_n-h_t\frac{V_n H_n}{2}\Big) Z_{i+1} = \Big( V_n Z_i + \frac{h_t}{2} \big( V_n H_n Z_i + F_{i+1} +F_{i} \big) \Big).
\end{aligned}
\end{equation*}
If we now let $n$ grow, $h_{n+1,n}$ will become zero and the projected method will converge towards the unprojected method. 
\subsection{Linearity of the methods} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The projection methods require a vector that can be used to generate the orthogonal space. In the general problem,
\begin{equation*}
\begin{aligned}
\dot{u}(t) &= Au(t) + b,
\end{aligned}
\end{equation*} 
$b$ is used to create the orthogonal space. But what if instead of just $b$, there where some time dependance, eg. $b + F(t)$, or (more illustrative)
\begin{equation*}
\begin{aligned}
\dot{u}(t) &= Au(t) + b F(t).
\end{aligned}
\end{equation*} 
In this case the projection method needs to be used two times, one time to solve $ \dot{u}_1(t) = Au_1 + b $, and another to solve $ \dot{u}_2(t) = Au_2 + F(t) $. $u_1(t)$ and $u_2(t)$ can then be added together to solve the original problem. \\
The reason for this is the need for a vector that can generate the orthogonal space. In this case there is no common vector $\tilde{b}$ so that $\tilde{b} \tilde{F}(t) = b + F(t)$. \\

\noindent An even bigger problem arises when a differential equation has a source term that is not separable in time and space, read more about this in \cite{elena} and \citep{min}. An equation that is separable in time and space can be written as $p(t,x,y) = g(x,y) \cdot F(t)$. If $p(t,x,y)$ is not separable, this is impossible. As an example, consider the wave equation \cite{waveequ}:
\begin{equation*}
\begin{aligned}
\frac{\partial^2 q(t,x,y)}{\partial t^2} = \frac{\partial^2 q(t,x,y)}{\partial x^2 } \frac{\partial^2 q(t,x,y)}{\partial y^2 } + g(t,x,y) \quad \text{ where } g(t,x,y) \neq p(x,y) F(t). \\
\end{aligned}
\end{equation*}
This equation cannot be discretized with a single vector $b$. If this is discretized to be on the form of \eqref{eqn:PDE} and solved with a projection method, it would look like:
\begin{equation*} \label{eqn:terrible}
\dot{u}(t) = A u(t) + \sum \limits_{i = 1}^{\hat{m}} e_i F_i(t),
\end{equation*}
where $\hat{m}$ is the size of the matrix, and $F_i(t)$ is a time dependent function after spacial discretization. The reason for this is that every different point, $x_i$ and $y_i$, gives a different time dependent function.%, and all of these functions need to be used to obtain the correct solution. \\
This means that equation \eqref{eqn:terrible} needs to be solved $\hat{m}$ times to obtain the solution. This gives the projection methods a terrible run time. It is not advised  to use any projection method if this is the case \cite{min}.
%\newpage


\subsection{Number of operations}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This section contains a brief discussion about computation times for the different methods presented.

%Since computation times might be uncertain due to different additional loads (web surfing, writing and so on), there will here be a short comparison between the number of operations for each method. This will also give a basis for what computationally complexities to expect from the different methods.\\

\begin{table}
\caption{Computational cost of some mathematical operations. $n$ is restart variable, $\hat{m}$ is the size of the large matrix, and $k$ is the number of steps in time. Numbers from this table is obtained from \cite{complex}. }
\centering
\begin{tabular}{l l }
Operation & Cost \\
\hline
Integration with forward Euler & $\mathcal{O}(k n^2)$ \\
Integration with Trapezoidal or midpoint rule & $\mathcal{O}(k n^3)$ \\
Arnoldi's algorithm & $ \mathcal{ O }(n^2 \hat{m})$ \\
Symplectic Lanczos method & $ \mathcal{O}(n^2 \hat{m}) $\\
Transforming from $z_n(t)$ to $u_n(t)$ & $ \mathcal{O}(\hat{m}nk) $\\
Matrix vector multiplication ($\hat{m}\times n$)(sparse matrix) & $ \mathcal{O}(\hat{m}) $ \\
Matrix vector multiplication ($\hat{m}\times n$) (dense matrix) & $ \mathcal{O}(n \hat{m}) $
\end{tabular}


\label{tab:cd}
\end{table}

\noindent A table of computational cost for different operations is given in Table \ref{tab:cd}. \\
Surprisingly the asymptotic cost for \texttt{KPM} and \texttt{SLM} is the same. A lot of small additional costs for \texttt{SLM} is hidden here, meaning that \texttt{KPM} probably has a smaller computation time, but the computation times are increasing similarly. The windowing is also a wildcard a since lot of costly operations are hidden. The difference between the projection methods and \texttt{DM} strongly depends on $n$ and $r_n$, making it impossible to conclude anything without actually doing it, except that all methods are linear with $k$. \\

\begin{table}
\caption{ Number of operations needed for the different methods with trapezoidal rule. $i_r$ is the number of restarts needed for the method to converge. For windowing $K\cdot k$ is equal to $k$ for the other methods. Windowing for \texttt{DM} is not interesting since it has exactly the same run time, and is exactly the same method. }
\centering
\begin{tabular}{l | l}
Method & Explonation and cost \\
\hline
\texttt{KPM} & Arnoldi's algorithm, an integration method, and the transformation.
\\ & $ \mathcal{O}((n^2 \hat{m} + k n^3 + \hat{m}nk)i_r)$ \\ 
\texttt{SLM} & Symplectic Lanczos method, an integration method and the transformation. 
\\ & $ \mathcal{O}((n \hat{m}^2 + k n^3 + \hat{m}nk)i_r) $  \\
\texttt{DM} & An integration method with $n = \hat{m}$. 
\\  & $\mathcal{O}(k\hat{m}^3)$ \\
Windowing  & \texttt{SLM} or \texttt{KPM} needs to be run $K$ times. \\ (\texttt{KPM} or \texttt{SLM}) & $\mathcal{O}((n \hat{m}^2 + k n^3 + \hat{m}nk)i_r K)$ \\
\end{tabular}
\label{tab:cc}

\end{table}

\section{\texttt{SLM} and its eigenvalue solving properties} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
If you do a quick internet search for "symplectic Lanczos method", most articles you find are in relation to symplectic Lanczos method's ability to approximate eigenvalues of Hamiltonian matrices. In this section I will try to explain what makes this algorithm so attractive for these types of problems. (This section is based on reading \cite{SLM1},\cite{SLM2}, \cite{SLM3}, and \cite{SLM4}.) \\

\noindent The eigenvalue problem is found in many different areas, eg. control theory, model reduction, system analysis. No other algorithm exploits the Hamiltonian and sparse structure of these matrices. \\

\noindent \texttt{SLM} is a relatively fast algorithm for transforming a big sparse Hamiltonian matrix to a smaller sparse Hamiltonian matrix with similar properties. Eigenvalues of Hamiltonian matrices comes in pairs, $\{ \pm \lambda \} $, or in quadruples, $\{ \pm \lambda $, $\pm \bar{\lambda} \} $. Since the projected matrix is Hamiltonian, it is easier to find these pairs or quadruples. An other important property is that the biggest, and therefore often most important eigenvalue, is found first.\\

\noindent The method is not without flaws. Frequent breakdown due to ill conditioned matrices occur. But due to its large potential much work has been done to overcome the difficulties. The most promising improvements are different types of restarts. This way the numerical estimations can be improved without losing the Hamiltonian properties, and without a too high cost.
Unfortunately not all are very efficient, and there is still much ongoing research on the topic. \\

\noindent An other method based on \texttt{SLM} is the \texttt{SR} - algorithm which has similarities with the \texttt{QR} - algorithm. \texttt{SR} applies symplectic operations to a Hamiltonian matrix instead of applying orthogonal operations as in \texttt{QR}. \\

\noindent Not all the papers are interested in improving the method, \cite{SLM4} shows under which assumptions the method converges. \\

\noindent Interestingly, many of these papers also compare \texttt{SLM} to \texttt{KPM}.


 
