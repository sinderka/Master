%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Explonation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
There will here be a short explanation of all solvers, constants, abbreviations and expressions used in this text. MATLAB notation is used where applicable.

\section{Projection methods}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As mentioned earlier, two projection methods will be used in this text: Arnoldi and SLM. The algorithm for the methods are given in Algorithm \ref{alg:arnoldi}, and Algorithm \ref{alg:symlanz} respectively, no theoretical derivation of the methods will be made in this text. The framework for both methods is given in Algorithm \ref{alg:PM}. \\

\begin{algorithm} [h!]
\begin{algorithmic} \caption{Arnoldi's algorithm\cite{arnold}} \label{alg:arnoldi}  
\STATE Start with $A \in \mathbb{R}^{\hat{m} \times \hat{m}}$, $v \in \mathbb{R}^{\hat{m}}$, $n \in \mathbb{N}$ and $\epsilon \in \mathbb{R}$.
\STATE $v_1 = v/\|v \|_2$
\FOR{$j = 1,2,\cdots, n $} 
   \STATE Compute $h_{i,j} =  v_iAv_j,v_i $ for $i = 1,2,\cdots, j$
    \STATE Compute $w_j = A v_j - \Sigma_{i=1}^{j} h_{i,j}v_i $
    \STATE $h_{j+1,j} = \| w_j \|_2$
    \IF{$h_{j+1,j} < \epsilon $} 
        \STATE\textbf{STOP}
    \ENDIF 
   \STATE $v_{j+1} = w_j/h_{j+1,j}$
\ENDFOR
\STATE Return $H$, $V$, $v_{n+1}$, $h_{n+1,n}$.
\end{algorithmic} 
\end{algorithm}

\begin{algorithm} \caption{Symplectic Lanczos method \cite{SLM}, with reortogonalization from \cite{SLMO}. } \label{alg:symlanz}
\begin{algorithmic}
\STATE Start with a Hamiltonian matrix $A \in \mathbb{R}^{2\tilde{m} \times 2 \tilde{m}}$, $\tilde{v_1} \in \mathbb{R}^{2 \tilde{m}}$, $\tilde{n} \in \mathbb{N}$
\STATE $v_0= 0 \in \mathbb{R}^{2 \tilde{m}}$
\STATE $\xi_1 = \| \tilde{v}_1\|_2$
\STATE $v_1= \frac{1}{\xi_1}  \tilde{v}_1$
\FOR {$j = 1,2, \cdots, \tilde{n}$}
	\STATE $v = A v_j$
	\STATE $\delta_j =  v_j^\top$
	\STATE $\tilde{w} = v-\delta_j v_j$
	\STATE $\kappa_j = v_j^\top J_{\tilde{m}} v $
	\STATE $w_j = \frac{1}{\kappa_j} \tilde{w_j}$
	\STATE $w = A w^j$
	\STATE $ \tilde{V}_{j-1} = [v_1,v_2,\cdots,v_{j-1},w_1,w_2,\cdots,w_{j-1}] $
	\STATE $ w_j = w_j + \tilde{V}_{j-1}J_{j-1} \tilde{V}_{j-1}^\top J_{\tilde{m}} w_j $
	\STATE $\beta = -w_j^\top J_{\tilde{m}} w$
	\STATE $\tilde{v}_{j+1} = w - \xi_j v_{j-1} - \beta_j v_j + \delta_j v_j$
	\STATE $ \xi_{j+1} = \|\tilde{v}_{j+1} \|_2 $
	\STATE $ v_{j+1} = \frac{1}{\xi_{j+1}} \tilde{v}_{j+1} $
	\STATE $ \tilde{V}_j = [v_1,v_2,\cdots,v_{j},w_1,w_2,\cdots,w_{j}] $
	\STATE $ v_{j+1} = v_{j+1} + \tilde{V}_j J_j \tilde{V}_j^\top J_{\tilde{m}} v_{j+1} $
\ENDFOR
\STATE $V = [v_1,v_2,\cdots,v_{\tilde{n}},w_1,w_2,\cdots,w_{\tilde{n}}]$
\STATE $H = \begin{bmatrix}
\text{diag} \big( [\delta_j]^n_{j=1} \big) & \text{tridiag}\big( [\xi_j]_{j=2}^n,[\beta_j]_{j=1}^n,[\xi_j]_{j=2}^n \big) \\
\text{diag} \big( [\kappa_j]^n_{j=1} \big) & \text{diag} \big( [-\delta_j]^n_{j=1} \big)
\end{bmatrix} $
\STATE Return $H$, $V$, $v_{n+1}$, $\xi_{n+1}$.
\end{algorithmic}
\end{algorithm}
Note that the relation between $\hat{m}$, $\tilde{n}$, $\tilde{m}$ used in the algorithms is given by $\hat{m} = 2\tilde{m}= 2(m-2)^2$ and $ n = 2\tilde{n}$. Don't worry to much about these details, it is just a way to simplify the expressions.

\begin{algorithm}[h]
\begin{algorithmic} \caption{Framework for the orthogonalisation methods\cite{min}} \label{alg:PM} 
\STATE Start with $A \in \mathbb{R}^{\hat{m} \times \hat{m}}$, $f(t)$, $v \in \mathbb{R}^{\hat{m}}$, $n \in \mathbb{N}$, a boolean value \texttt{restart}, an algorithm \texttt{alg}, $\epsilon \in \mathbb{R}$, and $i = 0$.
\STATE Compute $[V_n,H_n,h_{n+1,n}^i,v_{n+1}] = \texttt{alg}(A,v,n)$
\STATE Solve $  z_i'(t) = H_n z_i(t) + f(t) \| v \|_2 e_1  $ for $z_i(t)$
\STATE $ u_n(t) \leftarrow  V_n z_i(t) $
\STATE $ \delta = h_{n+1,n} $ 
\IF { \texttt{restart} == 1 }
	\WHILE{ $\epsilon < \delta$  } 
    		\STATE $i \leftarrow i + 1$
    		\STATE Compute $[V_n,H_n,h_{n+1,n}^i,v_{n+1}] = \texttt{alg}(A,v_{n+1},n)$
    		\STATE Solve $ z_i'(t) = H_n z_i(t) + h_{n+1,n}^{i-1}e_n^\top z_{i-1}(t)  $ for $z_i(t)$
    		\STATE $ u_n(t) \leftarrow u_n(t) + V_n z_i(t) $
    		\STATE $\delta = \max(u_n(t) - V_n z_i(t))$
	\ENDWHILE
\ENDIF
\STATE Return $u_n$.
\end{algorithmic} 
\end{algorithm}

If the solution is obtained without the use of an orthogonalisation, it will be denoted with DM for direct method.

The approximated solution, found by either SLM or Arnoldi or DM, will be denoted by $u^n$, where $n$ the same as the size as the orthogonal space used, $n$ will also be called a "restart variable". \\




\section{Zero initial condition}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For both KPM and SLM it is important that the initial conditions are zero. Equation \eqref{eqn:PDE} can be transformed so that it has zero initial conditions in the following way:
Start by shifting the solution
\begin{equation*}
\hat{u}(t) = u(t)-u_0,
\end{equation*}
then rewrite the original equation as
\begin{equation*}
\begin{aligned}
\dot{\hat{u}}(t) &= A \hat{u}(t) +A u_0 + F(t) \\
 \hat{u}(0)&= 0. \\
\end{aligned}
\end{equation*}
The equation above solves the shifted problem, solve the original problem by shifting it back with
\begin{equation*}
 u(t) &= \hat{u} + u_0. \\
\end{equation*}


All test problems with a non-zero initial condition will be transformed in this way.

\section{Discretization}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Divide each spacial direction in $m$ pieces, with step size $h_s = 1/m$. The number of steps in time is denoted by $k$, making the step size in time, $h_t = 1/k$.\\

Let the matrix $I_j$ be the identity matrix with dimension $j$, and let 
\begin{equation}
J_j = 
\begin{bmatrix}
0&I_j\\-I_j&0
\end{bmatrix}.
\end{equation}

Equation \eqref{eqn:PDE} can be the result of several discretized equations. Since SLM needs a Hamiltonian matrix this will be the main focus. Two different matrices was implemented, with some test problems.\\

The first is the 2 dimensional wave equation, 
\begin{equation}
\begin{aligned}
\frac{\partial^2 u}{\partial t^2} = \frac{\partial^2 u}{\partial x^2}+ \frac{\partial^2 u}{\partial y^2} + f(t,x,y).
\end{aligned}
\label{eqn:wave}
\end{equation}
This can be discretized to be on the form of equation \eqref{eqn:PDE}, with a Hamiltonian matrix
\begin{equation}
\begin{aligned}
\tilde{A} &= \frac{2}{h_s^2} \text{ gallery}('\text{poisson}', m-2) \\
A &= 
\begin{bmatrix}
 0 & I_{\hat{m}} \\ - \tilde{A} & 0 \\
\end{bmatrix}
\end{aligned}.
\end{equation}
The matrix $\tilde{A}$ is also known as the five-point stencil\cite{fivepoint}. This matrix will be referred to as \texttt{wave}. The second implemented Hamiltonian matrix is random, and given by
\begin{equation}
\begin{aligned}
\hat{A} &= \text{rand}(\hat{m}) \\
A &= \frac{1}{2} J_{\hat{m}} \cdot (\hat{A} + \hat{A}^\top + m^2 I_{\hat{m}}).
\end{aligned}
\end{equation}
Since we are interested in comparing the different projection methods to each other, the matrix will be saved and reused. This matrix will be referred to as \texttt{semirandom}. The part $m^2 I_{\hat{m}} $ is added to make $J_{\hat{m}}A$ diagonally dominant, since a fully random problem will not converge in general. The matrix is simulated as a 2 dimensional system. 

These two matrices also has some test problems that satisfies the conditions $$u(t,0,y) = u(t,1,y) = u(t,x,0) = u(t,x,1) = 0.$$ The test problems will be divided in two cases. One where the energy is constant, and one where the energy is varying. \\

In the case when the energy is constant and \texttt{wave} is used, the test problem is 
\begin{equation}
\begin{aligned}
u(t,x,y) &= \sin(\pi x) \sin(\pi y) \cos(\sqrt{2} \pi t) \\
u_0(x,y) &= \sin( \pi x) \sin(\pi y) \\
f(t,x,y) &= 0 ,
\end{aligned}
\end{equation}
and 
\begin{equation}
\begin{aligned}
u(t,x,y) &= \text{unknown} \\
u_0(x,y) &= \text{rand} (2 (m-2)^2,1) \\
f(t,x,y) &= 0
\end{aligned}
\end{equation}
for \texttt{semirandom}.\\ %This test problem is kept with the same conditions as the matrix.\\

In the case with varying energy and \texttt{wave} is used the test problem is 
\begin{equation}
\begin{aligned}
u(t,x,y) &= (x-1)x(y-1)y(t^2-t+1) \\
u_0(x,y) &= (x-1)x(y-1)y \\
f(t,x,y) &= 2 (y (y-1) + x (x-1)) \cdot \big( -(t^2-t+1) \big) ,
\end{aligned}
\end{equation}
and 
\begin{equation}
\begin{aligned}
u(t,x,y) &= \text{unknown} \\
u_0(x,y) &= 0 \\
f(t,x,y) &= \text{rand(1,k)} \cdot  \text{rand} (2 (m-2)^2,1), \\
\end{aligned}
\end{equation}
for \texttt{semirandom}. When using one of the projection methods $f$ needs to be separable in time and space. In the case where $f$ is not separable it is not recommended to use this method, see \cite{min} for more information.\\

The test problems are discretized with $y_i = i h_s$, $x_i = i h_s$ and $t_j = j h_t$ with $i = 1,2,\cdots,m-1 $ and $ j = 1,2,\cdots,k $. The time discretized solution of $u$ will be called $U$.

\section{K versus k}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
!!!!!Trenger bedre nanv!!!!!!!!!!!!!!!!!\\
!!!!!!!!!!!!!Må skrives om og slikt!!!!!!!!!!!!\\
The goal of this section is to compare two different approaches when solving for a large time domain.

Let $T_s$ denote simulated time, let $K$ be the number of pieces $T_s$ is divide into, and let $k$ be the number of pieces each $K$ is divided into. The procedure is to solve each of the $K$ intervals as separate problems, with the initial conditions updated. This is explained in a more precise manner in algorithm \ref{alg:Kversusk}.

\begin{algorithm} [h!]
\begin{algorithmic} \caption{!!!!!!!Spør elena om dette har et fint navn jeg kan bruke!!!!} \label{alg:Kversusk}  
\STATE Start with an initial value $U_0$, $K$ and $k$.
\STATE Make an empty vector $u$
\FOR{$j = 1,2,\cdots, K $} 
   \STATE Solve differential equation with $k+1$ points in time and initial value $U_0$
   \STATE Place the new points at the end of $u$.
   \STATE Update $U_0$ to be the last value of $u$ 
   \STATE Delete the last point of $u$
\ENDFOR
\STATE Return $u$.
\end{algorithmic} 
\end{algorithm}

\section{Energy}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The energy for a system on the form of equation \ref{eqn:PDE} is
\begin{equation}
%\frac{1}{2} e_r^{\top} J A e_r + e_r^\top J v_{n+1} e_{2\tilde{n}}^\top z
E(t) = \frac{1}{2} u(t)^\top J A u(t)
\label{eqn:energy}
\end{equation}

\section{Abbriviations}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Table \ref{tab:labels} contains an explanation of the expressions you will see on figures later.

\begin{table}[h]
\centering
\begin{tabular}{l| l}
$r_n$	& Number of restarts performed by Arnoldi or symplectic Lanczos method.  \\
$T_c$	& Computation time used to solv a problem \\
$er_1$ 	& Difference in error between analytical solution, and estimated solution. \\
$en_1$ 	& Difference in energy between analytical solution, and estimated solution. \\
$er_2$ 	& Difference in error between orthogonalised solution, and the\\& non-orthogonalised solution. \\
$en_2$ 	& Difference in energy between orthogonalised solution, and the\\& non-orthogonalised solution. \\
$m$ 		& Number of point in each spacial direction \\
$n$ 		& Size of orthogonal space, also called restart variable \\
$k$ 		& Number of points in time \\
$T_s$ 	& Simulated time \\
\texttt{restart}& A boolean value. If \texttt{restart} == 1, Arnoldi or \\&symplectic Lanczos method will restart. \\
$\epsilon$ & If \texttt{restart} is true, restarting will\\& commence until the change in the solution\\& is less than $\epsilon$ \\
\end{tabular}
\caption{ Explanation of abbreviations. }
\label{tab:labels}
\end{table}

\section{Implementation}
All algorithms, methods was made in matlab R2014b on an ubuntu 14.04 LTS computer with intel i7 4770 CPU and 16 FB of RAM.
